<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[我的编译器]]></title>
    <url>%2F2017%2F12%2F02%2F%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%2F%E6%88%91%E7%9A%84%E7%BC%96%E8%AF%91%E5%99%A8%2F</url>
    <content type="text"><![CDATA[我的编译器password: password 环境配置PG的配置VS属性的配置好了，这时在PG中的任务已经完成了，接下来想要使用VS来执行PG生成的程序，还需要进行一些属性的配置。多种组合可行。这里只说明了我自己用的一种组合，路径也以我的电脑为准。我的机器VS2015，win10。 在属性页面添加： 配置属性 -&gt; VC++目录 -&gt; 包含目录：D:/Program Files（x86）/Parser Generator 2/Cpp/Include 配置属性 -&gt; VC++目录 -&gt; 库目录：D:/Program Files（x86）/Parser Generator 2/Cpp/Lib C/C++ -&gt; 预处理器 -&gt; 预处理器定义：YYDEBUG,_MBCS; C/C++ -&gt; 代码生成 -&gt;运行库：多线程调试(/MTd) 链接器 -&gt; 输入 -&gt; 附加依赖项：ylmtd.lib; 是YYDEBUG,_MBCS;中间是逗号。 $$\color{red}{注意！上述除了”多线程调试“，其他均是在原有的基础上添加。}$$ 语法分析部分lex与yacc的结合lipeng的结合参考算了。没什么特别大的参考价值，他的着重点与我的不同。 oracle的文档：lex IBM的文档：lex–最终问题在这里面解决。查看其“结合”部分。 在lex中没有main函数，yylval是在lex中定义的一个变量，使用extern YYSTYPE yylval在yacc中使用。 “default”标签跳过“ ”的初始化操作编译器错误C2361:“default”标签跳过“ ”的初始化操作&amp;rd=true)“default”标签跳过“ ”的初始化操作解决 出现“default”标签跳过yylval的初始化操作的原因是：在lex中，我把rule section的正则语义动作定义段写在了对yylval的extract之前。只要挪到后面，这个错误就不见了。（这个看一看在cpp中的错误代码就能发现异常。） yylval：无法解析的外部符号写错地方了。yylval是在yacc里面的，写extern应该在lex里面写。 在yacc中无需用到yylval，这是因为在语义动作上，比如NUMBER，如果不写他的语义动作，他将会自动把产生式左部的值赋为与NUMBER一起给yacc的yylval的值。 成功！现在来总结一下叭！ 现在拥有：一个yacc文件名叫myparser.y，一个lex文件名叫mylexer.l。创建了一个新的既有lex又有yacc的项目。把原来lex中的除了main函数之外的东西拷贝到新程序的对应位置（不建议直接全部复制，因为可能会有细小问题出现。特别注意你在生成project的时候定义的lexer的名字！）注意上面提到过的那个rule section的问题。在lexer的include句之后定义extern string yylval（我这里使用string只是因为我的YYSTYPE是string，我的不能直接使用宏YYSTYPE）在lex的匹配语义动作中加上对yylval的赋值（yylval=yytext;这句将匹配对应的字符串给yylval。同样因为我的yylval是string，不是string可以自己转换一下。在lex的语义动作最后加上return NUBER;（NUBER只是一个例子，应该返回这个匹配对应的token。把原来yacc中的代码拷到新程序的对应位置（同样不建议直接全部复制。如果原来不懂yylavl的意义，在单token行也写了语义动作赋值，把这个语义动作删掉，或者使用\$1对这个值做你想要的处理之后赋给\$\$。好了！生成之后，在VS项目中运行就好啦！ 符号表的设计由于支持函数类等操作，所以要增加多个生存期作用域，需要多个符号表。这些符号表将动态生成，压进栈中。这也是考虑到作用域的特性。另一方面，由于作用域与语法相关，符号表的创建（除了全局符号表）在yacc中完成。yacc获取栈，lex获取栈，共同操作。 C++标准库栈上述链接其实没什么特别多的东西，记住两点三函数： 使用stack需要包括头文件#include&lt;stack&gt; 这是个模板类，使用时指定类型：stack&lt;CHash*&gt; mystack; 函数们：|函数名|作用||:–:|:–:||top()|返回栈顶元素，不弹出||pop()|弹出栈顶元素，不返回||push(member)|将member压栈||size()|返回栈中元素个数||empty()|检查栈是不是空| 简直是救星！string CString int char等互转 语法树的生成C++的文件操作 编译器对函数的处理YACC部分注意：yacc中的文法定义顺序是非常有关系的。不一样的顺序将会带来不一样的结果。越靠上的文法，越会先被匹配。在添加产生式的过程中出现了”rules are never reduced”。在第一次写作业时也出现了，但是那时没有好好积累。shift在这里是”移入“，reduce是”归约“。这是LR算法中的语言。 理解”rules are never reduced”-stackoverflow not getting reduced rules are never reduced 总结上述的一些： 可能是因为指出的那个没有在右边出现过。 可能是在前面的某个产生式包括了这个产生式的情况，所以不能被匹配。 你的递归没有办法被终止。 yacc似乎会将第一个产生式左部作为开始符号。 In general, you want to use the -v option to get yacc to prodcue a y.output file with detailed information about your grammar. This file will tell you specifically about all the conflicts and unused rules – what they are and how they come about – while the messages just give you a summary of the problems.(使用-v选项来获取更详细的输出信息。) 有时即使没有错误或者警告，程序也运行不出来，大概是匹配了不是本意的文法，自己看吧。我心情不好。一直感觉文法是对的，一直运行不出来。我的lex没有返回一些token给yacc。能运行就怪了。 没有好好理解VS里面.h与.cpp，不要随便在里面写一些函数，然后包括去使用。 LEX部分]]></content>
      <categories>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[琐碎]]></title>
    <url>%2F2017%2F12%2F02%2FUntitled-1%2F</url>
    <content type="text"><![CDATA[git的beanch原来真的可以在本地也切换啊！当我用git checkout lab3的时候，文件夹里的东西也会跟着变。这也就解释了，为什么我明明从写好的lab3复制文件到lab4的lab3却没反应，因为我根本就没有复制到la4文件夹的lab3分支！首先进入lab3再复制，最后进入lab4分支，指定merge lab3分支，成功！。 GIT分支 干什么 句子 删除本地的一个分支 git branch -d 新增加一个分支 git checkout -b 删除远程分支 git push -delete 我见过的错误解决方法 在 git merge lab3之后产生冲突，人为解决之后再次merge无法merge：只需根据提示： 123git add .git commit -m &quot;mesage&quot;git merge lab3 VScmd运行完之后闪退可以通过更改属性的方式解决。什么system(“pause”)啊也行吧，但是我觉得没有属性来的方便永久。 “配置属性”–&gt;“链接器”–&gt;“系统”，然后在右侧的列表中，在第一项”子系统“的值中选择”控制台（/SUBSUSTEM:CONSOLE）“。]]></content>
      <categories>
        <category>琐碎</category>
      </categories>
      <tags>
        <tag>all</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理——语法分析]]></title>
    <url>%2F2017%2F11%2F29%2F%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%2F%E8%AF%AD%E6%B3%95%E5%88%B6%E5%AF%BC%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[Tiny C中的语法树：节点分为两类 语句 表达式 语句还可以细分，表达式还可以细分。 注意，有些叶节点是不需要保存的。比如说if statement，if ( ) else都可以不需要保存。 具体创建树有两种，一种是递归下降的形式，另一种是yacc的形式，与语法制导定义相似。 （哇那我很多都写错了…这两天是白干了吗…）]]></content>
      <categories>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理——语法分析]]></title>
    <url>%2F2017%2F11%2F29%2F%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%2F%E8%AF%AD%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[语法分析终于开始语法分析的复习了！ 上下文无关文法：1. 便于扩充语言特性 2. 语法容易被理解 3. 检错机制加入（其他就不行？） 语法分析器的位置（等待加图） 通用的语法分析器几乎是不可能保证效率的（目前有通用的语法分析器，Cocke-Younger-Kasami），目前有两种算法，自底向上（对应最左推导LL），自顶向下（对应最右推导LR）。 语法错误处理在编程中可能会发生不同的错误，在词法（拼写错误）、语法（单词漏写、顺序错误）、语义、逻辑上都可能会发生错误，其中，语法错误相对较多，成为检查重点。 对语法错误的分析相对简单：它具有可行前缀/活前缀特性，在使用给定词法串前缀加上一些字符不能构成该语法的正确语言串时，就会发生错误。 处理错误有三个目标：清楚报告出现的错误、快速从错误中恢复、不能对正确程序的编译处理造成太大的影响。 给出一个报告错误的方法：打印出有问题的那行 -&gt; 给出一个错误所在地的指针 错误处理一般在于自圆其说。很难清晰的预测程序员原本的意图，只需要报告最一般的错误可能。 错误恢复策略Panic模式：遇到错误之后，不断丢弃输入中的符号。直到发现$\color{red}{同步词法单元}$集合中的符号。 $\color{red}{同步词法单元}$：通常是界限符，比如分号、右大括号。他们的意图清晰，没有二义性。编译器的设计这必须为自己的编译器定义合适的同步词法单元。 panic模式可能会丢掉很多正常输入（这些失去的分析有可能会影响接下来的分析），但是这种模式很简单，能够保证不会进入无限循环。接下来的几个方法都不能保证。 短语级：局部修正，继续分析。比如，可能会做一些逗号换分号，加减分号的操作。已经在一些修复型编译器中使用。 需要避免进入无限循环。比如设定是”在有错误的串前面加一个分号“，就可能会一直循环下去。 可以与panic结合，避免丢弃太多单词。 错误产生式：描述错误模式。 可以更好的进行修正，检测错误信息。 全局修正：x 上下文无关文法正式定义一个上下文无关文法由终结符、非终结符、一个开始符号、一组产生式组成。使用符号描述是：$(V{T} , V{N} , S , p)$ 在文法中有一些约定的命名方式（待补充） 推导实际上是语法等价的一个替换过程。使用右部替换左部。 形式化定义：$$\alphaA\beta \Rightarrow \alpha\gama\beta 当且仅当存在 A \rightarrow \gama$$ $$ $$ 推导与语言最左推导：总是替换最左边的非终结符 最右推导：总是替换最右边的非终结符 形式化定义： 推导与语法分析树一颗语法树可能会对应多个推导过程。如果限制了最左或者最右，那么一个语法树就只能对应一个推导。 CFG设计正则表达式可描述符号串$\in$GFG可描述字符串 NFA-&gt;CFG在CFG，与正则不同的是：CFG的开始符号是所有，正则的终态是空串；CFG的终态是空串，正则的终态是所有。 因此： 终态：$A_i \rightarrow \epsilon$ 形如xy(x!=y)的01串：（不能用正则） 奇数串：S-&gt;B|BSB B-&gt;0|1 偶数串：拆成两个穿拼接形式。只要两个奇数串中心位置不同，拼出的偶数串就符合上述描述。给出证明： 证明： CFG验证证明CFG G生成语言L：互相包含。 一个例子（略） CFG修改 去错 重写：满足特殊要求 $$不合要求的问题\begin{cases}二义性 \\epsilon-moves \回路 \左递归 \提取左公因子\end{cases}$$ 消除二义性]]></content>
      <categories>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lab3_Document]]></title>
    <url>%2F2017%2F11%2F27%2FLab3_Document%2F</url>
    <content type="text"><![CDATA[Lab3 DocumentPart A：User Enviornments and Exception Handling在这一部分，首先需要了解关于在本实验中env（进程）的相关定义： 12345678//in inc/env.henum &#123; ENV_FREE = 0, ENV_DYING, ENV_RUNNABLE, ENV_RUNNING, ENV_NOT_RUNNABLE&#125;; 这是在本实验中定义的进程状态，在实验中，仅用到了ENV_FREE、ENV_RUNNABLE、以及ENV_RUNNING。 类似于课上讲的pcb，在本实验中对一个进程的描述定义如下： //in inc/env.c 第一个变量env_tf是当中断或异常或系统调用发生的时候，该进程需要保存的寄存器的值；最后一个变量指明了该进程的地址空间。 在kern/env.c中可以看到，关于进程，有三个全局的变量： 123struct Env *envs = NULL;// All environmentsstruct Env *curenv = NULL;// The current envstatic struct Env *env_free_list;// Free environment list 在马上进行的初始化之后，在envs所标识的物理地址开始，一直到NENV\*(struct Env)结束，是所有可以被使用的进程描述结构占用的空间，每一个(struct Env)大小都是一个可用的Env描述。curenv被指示为是当前正在运行的进程。env_free_list指向第一个可以使用的进程描述结构，这样的设计使得进程描述结构的申请与销毁都变得很容易，而且由于该链表中存储的是处于ENV_FREE状态的进程，很少需要有添加删除的过程。 Allocating the Environments Array就像在实验二中那样，在这里需要修改pmap.c中的代码，为envs映射空间。仿照在lab2中的写法，修改程序如下： 12//in kern/pmap.c the first LAB3: Your code hereenvs=(struct Env\*)(boot\_alloc(NENV\*sizeof(struct Env)));//为Env结构申请空间 1234//in kern/pmap.c the second LAB3: Your code herefor(int i=0;i&lt;ROUNDUP(NENV\*sizeof(struct Env),PGSIZE);i+=PGSIZE)&#123; page_insert(kern_pgdir,(struct PageInfo\*)pa2page(PADDR(envs)+i),(void\*)(UENVS+i),PTE_U); &#125; 像在lab2中做过的那样，boot\_map\_region使得从PADDR(envs)开始，连续PTSIZE的物理空间映射到从UENVS开始的虚拟地址空间上。 做到这里尝试make qemu的时候，发现失败了。 注意：回想在lab2中所做的工作，在page_init函数中曾经为没有用的页面加入freelist。查看当时的代码发现，那时计算剩余空间的时候，是从为npages分配空间呢的后一页开始计算的。在mem_init中修改了之后的映射之后，这时的freelist应该从envs占用所有空间之后的第一页进行添加，于是修改代码如下： 1234567int begin=(int)ROUNDUP(((char*)envs) + (sizeof(struct Env) * NENV) - 0xf0000000, PGSIZE);for (i=begin/PGSIZE;i&lt;npages;i++)&#123; pages[i].pp_ref=0; pages[i].pp_link=page_free_list; page_free_list=&amp;pages[i];&#125; 再次make qemu，得到结果如下： Creating and Running Environments在这一部分，将会完善kern/env.c中的内容，来建立起一个进程操作的雏形。 由于还没有文件系统，这时我们用一个技巧来骗过boot loader，使得bootloader加载kernel的时候也把我们需要的用户程序加载进来。这些用户程序在user文件夹下。 首先， -b binary option告诉编译器只需要将程序编译成二进制文件，而不需要生成.o文件，随后把这个文件作为一个较大的数组放在最后，同时在真正应该被搬进内存的文件中声明extern变量，告诉编译器在链接之前先不要管这个。随后，在链接阶段，大数组已经被load进来，与其他代码一同进行地址映射。最终这里的代码可以被执行。 查看kern/init.c文件，发现在所有初始化之后有这样的语句： 1234567#if defined(TEST) // Don't touch -- used by grading script! ENV_CREATE(TEST, ENV_TYPE_USER);#else // Touch all you want. ENV_CREATE(user_testbss, ENV_TYPE_USER);#endif 这是告诉编译器：如果用户采用make qemu的方式运行，那么就创建一个user_testbss这样的用户进程，实际上就是执行testbss.c文件生成的那个binary。 在env完成之后对这里进行更深一步的讨论。 接下来需要写出下面几个函数的代码： env_init env_setup_vm region_alloc load_icode env_create env_run 12345678910111213141516171819202122232425262728293031// Mark all environments in 'envs' as free, set their env_ids to 0,// and insert them into the env_free_list.//把所有的envs设置成free的，他们的id都是0，然后插入freelist// Make sure the environments are in the free list in the same order// they are in the envs array (i.e., so that the first call to// env_alloc() returns envs[0]).//需要保证所有在freelist里面的env的顺序与在数组中的env相同？？？////本函数：初始化所有在env数组中的的env结构，把它们加到freelist里面，要求如上。//在meminit之后执行。//env数组：*envs，当前：*curenv，freelist：*env_free_listvoidenv_init(void)&#123; struct Env*current=NULL; int i=NENV-1; //一共NENV个env while(i&gt;=0)&#123; //NENV is defined in inc/env.h //current-&gt;env_tf=3; envs[i].env_id=0; envs[i].env_runs=0;// current-&gt;env_pgdir=NULL; envs[i].env_link=env_free_list; env_free_list=&amp;envs[i]; i--;&#125; // Per-CPU part of the initialization env_init_percpu(); cprintf("envinit done!\n");&#125; 这个函数比较简单，需要注意的一点是要求倒着添加到链表里面。其他只是按照要求写的代码。 这个函数在最后执行了一个已经完成的函数：env_init_percpu，如下图： 该函数完成对各个段寄存器的设置。 1234567891011121314151617181920212223242526272829303132333435363738//为进程e初始化内核虚拟空间static intenv_setup_vm(struct Env *e)&#123; int i; struct PageInfo *p = NULL; // Allocate a page for the page directory if (!(p = page_alloc(ALLOC_ZERO))) return -E_NO_MEM; // Now, set e-&gt;env_pgdir and initialize the page directory. //初始化pagedir？ // Hint: // - The VA space of all envs is identical above UTOP // (except at UVPT, which we've set below). // See inc/memlayout.h for permissions and layout. // Can you use kern_pgdir as a template? Hint: Yes. // (Make sure you got the permissions right in Lab 2.) //所有envs的虚拟地址空间都在UTOP之上 //查看inc/memlayout.h看许可以及布局 //可以使用kern_pgdir作为一个模板 // - The initial VA below UTOP is empty. // - You do not need to make any more calls to page_alloc. // - Note: In general, pp_ref is not maintained for // physical pages mapped only above UTOP, but env_pgdir // is an exception -- you need to increment env_pgdir's // pp_ref for env_free to work correctly. // - The functions in kern/pmap.h are handy. // LAB 3: Your code here. e-&gt;env_pgdir=(pde_t*)page2kva(p);//page2kva in pmap.c p-&gt;pp_ref++; memcpy(e-&gt;env_pgdir,kern_pgdir,PGSIZE);//kernel's e is the kernel's e //permission:RR //p[PDX(UTOP)] = PADDR(p) | PTE_U | PTE_P; // UVPT maps the env's own page table read-only. // Permissions: kernel R, user R e-&gt;env_pgdir[PDX(UVPT)] = PADDR(e-&gt;env_pgdir) | PTE_P | PTE_U; return 0;&#125; 123456789101112131415161718192021//为进程申请len字节的物理地址空间，然后把它map到vastatic voidregion_alloc(struct Env *e, void *va, size_t len)&#123; // LAB 3: Your code here. // (But only if you need it for load_icode.) // // Hint: It is easier to use region_alloc if the caller can pass // 'va' and 'len' values that are not page-aligned. //如果允许传入不是页对齐的va（虚拟地址）和len（长度）的话会更好一些 // You should round va down, and round (va + len) up. // (Watch out for corner-cases!) //什么叫corner-cases //获取首地址 //只是申请了页面，其实没有真正的物理空间 for(int i=(uint32_t)ROUNDDOWN(va,PGSIZE);i&lt;(uint32_t)ROUNDUP(va+len,PGSIZE);i+=PGSIZE)&#123; struct PageInfo*pa=page_alloc(0); if(pa==NULL)panic("fail to alloc !in region_alloc!"); page_insert(e-&gt;env_pgdir,pa,(void*)i,PTE_U|PTE_W); &#125;&#125; 该函数指定进程申请从指定虚拟地址开始的len字节空间。由于vs与va+len可能不是页面对齐的，因此真正在分配页面的时候，需要做操作ROUNDOWN与ROUNDUP来进行页面的对齐。当申请页面失败时，panic，否则就将页面插入到当前进程的pgdir中。 1234567891011121314151617181920212223242526272829303132333435363738static voidload_icode(struct Env *e, uint8_t *binary)//binary：ELF&#123;//注意，对应的用户数据已经在内存中了 struct Elf*elf=(struct Elf*)binary;//给定的ELF文件头结构（更详细的，查看inc/elf.h） struct Proghdr *ph, *eph;//ELF文件指定程序段的程序头 // is this a valid ELF? if (elf-&gt;e_magic != ELF_MAGIC) panic("not a elf!"); ph = (struct Proghdr *) ((uint8_t *) elf + elf-&gt;e_phoff);//第一个程序头开始 eph = ph + elf-&gt;e_phnum;//最后一个不是程序头结构开始的位置 lcr3(PADDR(e-&gt;env_pgdir));//将CR3中装载进去这次的页目录 for (; ph &lt; eph; ph++)&#123; if(ph-&gt;p_type==ELF_PROG_LOAD)&#123; region_alloc(e,(void*)ph-&gt;p_va,ph-&gt;p_memsz); //申请从ph-&gt;p_va程序假设自己所在的虚拟地址开始，到va+memsz结束的空间 memmove((void*)ph-&gt;p_va,binary+ph-&gt;p_offset,ph-&gt;p_filesz); // The ph-&gt;p_filesz bytes from the ELF binary, starting at // 'binary + ph-&gt;p_offset', should be copied to virtual address // ph-&gt;p_va. Any remaining memory bytes should be cleared to zero. // (The ELF header should have ph-&gt;p_filesz &lt;= ph-&gt;p_memsz.) // Use functions from the previous lab to allocate and map pages. //在ELF中的ph-&gt;p_filesz字节的，从binary+ph-&gt;p_offset都应该被copy到虚拟地址空间ph-&gt;p_va //任何剩余的空间被清零 memset((void*)(ph-&gt;p_va+ph-&gt;p_filesz),0,ph-&gt;p_memsz-ph-&gt;p_filesz);//eme2file //memcpy((void*)ph-&gt;p_va,binary+ph-&gt;p_offset,ph-&gt;p_filesz); &#125; &#125;//after modification , everything is OK! // call the entry point from the ELF header // note: does not return! //((void (*)(void)) (ELFHDR-&gt;e_entry))(); e-&gt;env_tf.tf_eip=elf-&gt;e_entry; // Now map one page for the program's initial stack // at virtual address USTACKTOP - PGSIZE. // LAB 3: Your code here. region_alloc(e,(void*)(USTACKTOP-PGSIZE),PGSIZE); lcr3(PADDR(kern_pgdir));&#125; 该函数负责把已经装入内存的binary文件重新装载到ELF文件头中指定的位置，并为用户进程创建一个栈。 123456789101112voidenv_create(uint8_t *binary, enum EnvType type)&#123; // LAB 3: Your code here. struct Env *newenv_store; int result=env_alloc(&amp;newenv_store,0); if(result&lt;0)panic("env_alloc: %e", result); ; if(result==0)&#123; newenv_store-&gt;env_type=type; load_icode(newenv_store,binary); &#125;&#125; 这里使用到了已经定义好的函数：env_alloc。env_create本身比较简单，我们来看一下env_alloc： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647intenv_alloc(struct Env **newenv_store, envid_t parent_id)&#123; int32_t generation; int r; struct Env *e; //没有新的可以用了 if (!(e = env_free_list)) return -E_NO_FREE_ENV; // Allocate and set up the page directory for this environment. if ((r = env_setup_vm(e)) &lt; 0) return r; // Generate an env_id for this environment. generation = (e-&gt;env_id + (1 &lt;&lt; ENVGENSHIFT)) &amp; ~(NENV - 1); if (generation &lt;= 0) // Don't create a negative env_id. generation = 1 &lt;&lt; ENVGENSHIFT; e-&gt;env_id = generation | (e - envs); // Set the basic status variables. e-&gt;env_parent_id = parent_id; e-&gt;env_type = ENV_TYPE_USER; e-&gt;env_status = ENV_RUNNABLE; e-&gt;env_runs = 0; // Clear out all the saved register state, // to prevent the register values // of a prior environment inhabiting this Env structure // from "leaking" into our new environment. memset(&amp;e-&gt;env_tf, 0, sizeof(e-&gt;env_tf)); // Set up appropriate initial values for the segment registers. // GD_UD is the user data segment selector in the GDT, and // GD_UT is the user text segment selector (see inc/memlayout.h). // The low 2 bits of each segment register contains the // Requestor Privilege Level (RPL); 3 means user mode. When // we switch privilege levels, the hardware does various // checks involving the RPL and the Descriptor Privilege Level // (DPL) stored in the descriptors themselves. e-&gt;env_tf.tf_ds = GD_UD | 3; e-&gt;env_tf.tf_es = GD_UD | 3; e-&gt;env_tf.tf_ss = GD_UD | 3; e-&gt;env_tf.tf_esp = USTACKTOP; e-&gt;env_tf.tf_cs = GD_UT | 3; // You will set e-&gt;env_tf.tf_eip later. // commit the allocation env_free_list = e-&gt;env_link; *newenv_store = e; cprintf("[%08x] new env %08x\n", curenv ? curenv-&gt;env_id : 0, e-&gt;env_id); return 0;&#125; 该函数完成一系列的初始化操作，申请新进程，地址空间申请，一些属性的设置（这也是在env_init中不是做设置的原因），寄存器值的清理与设置等等。 123456789101112131415voidenv_run(struct Env *e)&#123; if (curenv != e) &#123; // if (curenv-&gt;env_status == ENV_RUNNING) // curenv-&gt;env_status = ENV_RUNNABLE; curenv = e; e-&gt;env_status = ENV_RUNNING; e-&gt;env_runs++; lcr3(PADDR(e-&gt;env_pgdir));//将当前的地址空间设置为当前进程的地址空间 &#125; env_pop_tf(&amp;e-&gt;env_tf); //panic("env_run not yet implemented");&#125; 该函数只需要根据提示即可写出，看一下env_pop_tf的代码： 1234567891011121314151617181920212223voidenv_pop_tf(struct Trapframe *tf)&#123; __asm __volatile("movl %0,%%esp\n" "\tpopal\n" "\tpopl %%es\n" "\tpopl %%ds\n" "\taddl $0x8,%%esp\n" /* skip tf_trapno and tf_errcode */ "\tiret" : : "g" (tf) : "memory"); //对应的汇编代码： /* mov tf,eax movl eax,esp popal popl es popl ds addl $0x8,esp iret */ //从这里开始，真正退出了内核态，进入用户态 panic("iret failed"); /* mostly to placate the compiler */&#125; 真正开始运行时，是需要trap伪装栈内容之后才能调用该函数，进入用户态的。trap的内容将在PartB讨论。届时还会需要用到该函数的内容。 按照文档的说明，使用b env_pop_tf打断点，查看内容，在经过若干次执行之后： iret将进行退栈操作，此处暂未涉及。 这里写完之后，就可以梳理一下在进入用户态之前都发生了什么：(in kern/init.c) 首先，清理bss。这将保证所有未初始化的都是0，在testbss中将会有体现。然后初始化控制台，进行内存的初始化，进程初始化，trap的初始化。随后创建一个env，开始执行。 现在，我们可以讨论在上面提到过的ENV_CREATE。 ENV_CREATE在kern/env.c中被定义： 首先可以看到本文件对那些骗boot loader装载进来的用户程序的外部定义，随后调用env_create函数，以ENV_PASTE3(_binary_obj_, x, _start)作为传入的ELF文件头结构，以type作为类型被创建。 随后，在create之后，执行load_icode，将对应内存的文件搬运到指定内存，并设置进程的入口点为函数的入口点，为用户进程初始化map一个页空间的栈，准备开始执行。 随后，调用了env_run函数，执行这个已经准备好的用户程序。但是在这个阶段，这个程序实际上还不能被真正执行。 Handling Interrupts and Exceptions中断与异常都将会打断原有程序的执行，转而执行其他程序。他们的区别就在于，中断用于处理那些处理器外部的异步事件，而异常处理被处理器在执行指令时发现的错误情况。 中断会被INTR引起，异常会被INT引起。这也确定了breakpoint应该是一个异常而不是中断。 Basics of Protected Control Transfer中断和异常都可能完成这个转化，完成特权级的转化需要两个东西：IDT与TSS。 IDT：IDT确保了用户程序切换到内核态的情况只有已经由kernel定义的几种，保证切换过程中的安全性。 x86允许256个不同的中断或异常，他们有着不一样的中断向量，CPU利用中断向量来确定这个中断的中断向量表，进而找到处理中断的入口，执行处理程序。找到中断向量的描述符之后，EIP中装载的将会是中断处理程序的入口地址，CS中装载的是运行优先级。不一样的运行优先级设置带来不一样的结果，在接下来的实验中将会被讨论到。 一个IDT Gate是这样的： segment selector与offset用来描述在段机制下自己的中断响应函数的位置。segment selector负责在GDT里面找到对应的段，offset负责找对应的偏移量。找到函数之后，保存现场并执行响应函数。 TSS：处理器需要空间来存储旧的寄存器中的值，这样从中断或异常中返回的时候，用户程序才能够接着执行。因此，在从用户态切换到内核态的时候，需要换一个栈进行操作。这时就需要TSS来确定段寄存器来确定这个栈到底在哪里。从用户态切换到内核态时，首先在栈中push一些寄存器值： 然后加载中断描述符中的值，然后把ESP和SS置为指向新栈的值。 尽管TSS可能会很复杂，但是在这个实验中只是用到了指向内核栈的部分。 在中断发生的时候，系统将会看一下产生中断的程序的特权级，决定应该怎样保护现场。如果在特权状态，不用换栈，只需要保存更少的东西，而处于用户状态下的程序在执行完中断响应函数之后还要从内核栈退回到自己的用户栈，保存了更多的东西。 iret：系统指令，从中断中返回。ret：从函数中返回retf：与ret基本一样，从函数中返回。（弹出栈，拿出返回地址（就是下一条语句）返回值，丢掉压栈参数） Types of Exceptions and Interrupts当一个进程在User Environment运行时，忽然遇到了一个中断： 进程切换到被TSS定义的那个栈，在JOS中SS0是GD_KD（GDT中的kernel code），ESP0是KSTACKTOP。 进程把一些值push到这个kernel栈里面。 找到IDT entry，开始执行函数 需要注意的是，就像上面已经讨论过的，有些中断/异常会push error code，而有的不会。 如果是在内核态发生了中断，就不需要进行换栈操作，这时旧的SS与ESP不会被保存。 怎么从内核态切换到用户态呢，其实方法是相似的。新开一个用户栈，存上中断以为的数据，随后iret，这样就降到了用户模式。 Setting Up the IDT现在来查看本实验中关于中断/trap的内容。 在inc/trap.h中定义了许多中断号，PushRegs结构以及TrapFrame结构。 TrapFrame结构中定义了trapnumer、es、ds等在值，这些值在特权级发生改变的时候会被压栈出栈保存，在后面还会用到。 为了完成Exercise4，首先分析trap.c以及trapentry.S中的代码： 这两个macro将会帮我们进行trapno的压栈操作，函数名的定义。第一个用于自动压栈errorcode的，后一个处理不自动压栈errorcode的。找到每个中断类型的压栈方式： 这时应该明确，这时仍处于原来的特权级。 于是，使用上述两个macro来为trap添加一个入口点，这部分的代码： 然后考虑/kern/trap.c中的内容，在这之前，首先查看macro SETGATE的作用： 1234567891011121314//in inc\mmu.h#define SETGATE(gate, istrap, sel, off, dpl) \&#123; \ (gate).gd_off_15_0 = (uint32_t) (off) &amp; 0xffff; \ (gate).gd_sel = (sel); \ (gate).gd_args = 0; \ (gate).gd_rsv1 = 0; \ (gate).gd_type = (istrap) ? STS_TG32 : STS_IG32; \ (gate).gd_s = 0; \ (gate).gd_dpl = (dpl); \ (gate).gd_p = 1; \ (gate).gd_off_31_16 = (uint32_t) (off) &gt;&gt; 16; \&#125; 这个macro帮我们设置了IDT的Gate。 于是，根据前面对TRAPHANDLER_NOEC与TRAPHANDLER的描述以及SETGATE，写trap_init如下： 接下来考虑_alltraps。根据提示，写出如下代码： 提示信息： Your _alltraps should: 1. push values to make the stack look like a struct Trapframe 2. load GD_KD into %ds and %es 3. pushl %esp to pass a pointer to the Trapframe as an argument to trap() 4. call trap (can trap ever return?) Consider using the pushal instruction; it fits nicely with the layout of the struct Trapframe. 代码： 进行makegrade检测： Answer the following questions in your answers-lab3.txt: 1. What is the purpose of having an individual handler function for each exception/interrupt? (i.e., if all exceptions/interrupts were delivered to the same handler, what feature that exists in the current implementation could not be provided?) 2. Did you have to do anything to make the user/softint program behave correctly? The grade script expects it to produce a general protection fault (trap 13), but softint&apos;s code says int $14. Why should this produce interrupt vector 13? What happens if the kernel actually allows softint&apos;s int $14 instruction to invoke the kernel&apos;s page fault handler (which is interrupt vector 14)? 回答： 如果所有的中断都是一个响应函数，那么首先需要统一是不是要自动压栈errorcode，其次在SETGATE时就无法指定特权级，就不能为内核中断提供不同的保护。比如syscall允许用户程序产生，但是诸如divideerror等只可以由硬件产生，如果所有都是用同一个优先级，很容易被别有用心的用户程序取得内核权限。 pagefault14不允许用户程序直接发起，必须经由硬件产生。这种机制可以更好地保护内存。假如允许用户程序自己产生int14，则每引发一个缺页中断系统就需要分配一个虚拟页，可能会被恶意程序利用，使得内存崩溃。而int13则是general protection interrupt，保护自己不去管那些不允许用户自己产生的中断。如果希望int14可以正常被用户程序直接产生，应该将其特权级设置为3. Page Faults, Breakpoints Exceptions, and System CallsHandling Page Faultspage fault是14号中断。当处理器捕获到14号中断的时候，它会在CR2中存储一个引起pagefault的线性地址。下面来处理这个中断。 在上一步分析到，在把es与ds分别都设置为GD_KD之后，这时调用trap函数，我们先来分析trap函数。 FL_IF是在inc/mmu.h中定义的中断标志。如果寄存器eflage中中断标志没有被置位，则不是一个中断。 在tf结构tf_cs的低位存储着优先级，假如优先级是3，则是在用户态。这时是无权处理中断函数的，应该首先升级成为内核态，然后再处理。然后调用trap_diapatch，来分发处理各种不同的trap，处理完毕回来之后，如果没有什么异常就可以接着运行。 在trap_dispatch中添加下面的代码： 1234if(tf-&gt;tf_trapno==T_PGFLT)&#123; page_fault_handler(tf);//此时该函数尚未完成 return;&#125; 这时去查看page_fault_handle，此函数尚未完成，但是对于用户态的trap已经给出了解决，可以看到一句代码fault_va = rcr2();。这就是前面提到的在CR2中储存引起pagefault的线性地址，随后的打印操作即可以打印出这事的信息。 The Breakpoint Exception其实，breakpoint也只是一个异常，函数入口点的相关代码在前面都已经写出来了。需要说的是，在我最早使用breakpoint作为该函数的名字时，运行make run-breakpoint一直不成功。如果breakpoint还有其他用处，希望可以发现。目前暂时搁置这个问题。 这里需要做的只是补充trap_dipatch，代码如下： 1234if (tf-&gt;tf_trapno == T_BRKPT) &#123; monitor(tf); return;&#125; Questions 1. The break point test case will either generate a break point exception or a general protection fault depending on how you initialized the break point entry in the IDT (i.e., your call to SETGATE from trap_init). Why? How do you need to set it up in order to get the breakpoint exception to work as specified above and what incorrect setup would cause it to trigger a general protection fault? 2. What do you think is the point of these mechanisms, particularly in light of what the user/softint test program does? 回答： 这个问题的答案与上一个相似。当特权级设置为0的时候，只允许硬件产生中断而不会允许用户自己去调用。 更好的保护机制。 makegrade信息将在最后一同贴出。 System calls在JOS中，系统调用定义的中断号是48号，系统调用不能由硬件产生，因此需要允许用户程序来生成系统调用。这也是在SETGATE中最后一个参数是3的原因。system call在初步判断时与trap处理相同，但是处理过程更加复杂。应用程序将会把系统调用的号码以及参数放在寄存器里面，这样内核将不需要在用户环境的堆栈或指令流中找数据。sysno将会在寄存器eax之中，其他参数将会在edx , ecx , ebx , edi , esi之中。返回时的返回值将会存在eax之中。 首先查看lib/syscall.c，syscall的代码以及对应的汇编代码如下： 补充kern/syscall.c以及kern/trap.c，代码如下： 这里需要提出的是，在kern/syscall.c中有另一个未完成的函数sys_cput，这个函数要求检查内存是否可以被访问，而在后面的练习中有相关函数的补充，在这里先不贴出代码。 User-mode startup用户程序开始运行时是在lib/entry.S，随后在lib/libmain.c中call libmain()。在lib/entry.S中可以看到： envs已经被定义了，因此下面进入lib/libmain.c来初始化thisenv。根据提示，查看inc/env.h中的ENVX，其定义如下： 1#define ENVX(envid) ((envid) &amp; (NENV - 1)) 为了得到当前的进程号，可以调用在kern/syscall.c中的sys_getenvid函数，该函数如下： 12345static envid_tsys_getenvid(void)&#123; return curenv-&gt;env_id;&#125; 但是，根据inc/env.h，env_id并不是一个完全的envs中的序号， 根据注释，ENVX(eid)才是真正在envs中的偏移，于是，在libmain中的代码如下： 1thisenv = envs+ENVX(sys_getenvid()); 其实，只有在这部分完成之后，才能正确的调用用户函数。这是因为在退出时需要访问thisenv-&gt;env_id，而先前这个变量并没有值。 在用户程序执行完成之后，就会调用sys_env_destory来退出自己。 Page faults and memory protection内存保护是操作系统的一个重要特性，确保一个程序中的错误不会破坏其他程序或破坏OS本身。OS通常依赖于硬件来保护自己，他知道哪块虚拟地址可用，哪块不可用，如果一个程序想要访问他没有权限访问的地址，会引发错误。如果这个错误可以被解决，OS会试图解决，如果不能解决，犯错的进程就不可以再运行了。 系统调用为内存保护提出了一个有趣的问题。大多数系统调用接口让用户程序将指针传递给内核。这些指针指向要读取或写入的用户缓冲区。内核然后在执行系统调用的时候去引用这些指针。这有两个问题： 1.内核中的页面错误可能比用户程序中的页面错误严重得多。如果内核页面在处理自己的数据结构的时候出错，这是一个内核错误，而且错误处理程序应该让内核（也就是整个系统）崩溃。但是当内核解引用用户程序给它的指针时，它需要一种方法来记住任何页面错误，这些解除引用实际上是代表用户程序。 2.内核通常拥有比用户程序更多的内存权限。用户程序可能会传递一个指向系统调用的指针，指向内核可以读或写的内存，但是程序不能。内核必须小心，不要被欺骗引用这样一个指针，因为这可能会泄露私有信息或破坏内核的完整性。 下面将处理对用户读写指针的访问权限处理，如果用户想要读写的地方表示允许，就允许，否则不允许。而内核，如果出现了page fault，立即崩溃。 在kern/trap.c中添加对内核发生了pagefault的处理，这也是上面提到的尚未完成的操作。根据提示，由于tf_cs中的低位代表特权级，直接查看特权级：如果不是在用户态发生的，就直接panic。代码如下： 12if ((tf-&gt;tf_cs&amp;3) == 0) panic("Kernel page fault!"); 接下来对kern/pmap.c中的user_mem_check以及user_mem_assert进行操作： 只需要按照说明检查内存中位置以及PTE_P和PTE_U等即可，直接贴出代码： 下面是user_mem_assert的代码。 12345678910111213//检查进程env是不是允许访问这个区域//如果可以就return//不行的话env就死了，如果env是当前运行进程，就不返回了//user_mem_checkvoiduser_mem_assert(struct Env *env, const void *va, size_t len, int perm)&#123; if (user_mem_check(env, va, len, perm | PTE_U) &lt; 0) &#123; cprintf("[%08x] user_mem_check assertion failure for " "va %08x\n", env-&gt;env_id, user_mem_check_addr); env_destroy(env); // may not return &#125;&#125; 最后，对在实验1中写过的debuginfo_eip进行更改，使得backtrace可以使用。将代码贴出： 本实验的基础部分到这里结束。 最终的makegrade： CHALLENGE： 完成了单步执行的challenge。 根据提示，查找EFLAGS信息，得到这样的信息： 于是为了能够单步执行，z在monitor中修改如下部分： 添加指令si 写函数mon_si并在.h中声明（声明略去） 修改kern/trap.c，使得能够处理debug的中断信息 运行： 类似的，就可以加上c指令使继续运行到下一个breakpoint。 添加指令c 写函数mon_c 只是，这时需要将TF位置为0。 结果： 为了更好地查看si与c的结果，我将breakpoint.c修改为下： 最后，再次运行make grade： 本实验完成。代码已经提交到git.mobisys.cc。 点击链接预览lab3工程代码~ 如果想要直接下载的话就点这个（这是百度的网盘） yayi2456 &lt;(￣︶￣)↗[GO!]&gt; 最后，列出那些给过我帮助的网站！谢谢！（也便于我以后找到！ 一个学霸的自我修养 clann24 clann的实际代码与README代码不一样啊，而且明明clann的自己可以运行，加到我的代码上却不能运行了。对clann的load_icode存疑。 朴实中透露着惊喜 mick_seu 可以说是很厉害了。代码简单易懂，正确率超高，超适用。 学习并快乐着 飞龙 飞龙的代码也很简单易懂！而且一看就是自己写的，与网上主流的都不一样。 X86 CPU的EFLAGS寄存器各个标识位 伊凡 How to read and write x86 flags registers directly? stackoverflow 我需要BB一句。到了OS课的该是大三了。stackoverflow这个社区真的是强推，无数问题都是在这个社区找到的回答。嗯，强推。 markdown to html md转html在线 emm其实效果一般…很不好….但是，毕竟在线，不想用的话，自己安装pandoc，费劲费空间。 另外一个html与md markdown的chrome插件]]></content>
      <categories>
        <category>OS</category>
        <category>MIT Lab</category>
      </categories>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络概述]]></title>
    <url>%2F2017%2F11%2F21%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[这是第一次课！时隔许久，也算是复习啦~ # 网络边缘网络核心分组交换在各种网络之中，端对端之间互相传送报文。为了从源系统项目地端系统发送报文，源端系统将报文分成若干的小数据块，成为分组。每个分组都通过源端系统与目的端系统之间的分组交换机（主要是路由器以及链路交换机）传送。分组以等于链路最大传输速率的速度通过通信链路，因此如果传输速度是R比特/s，那么传输L比特分组的时间是L/R比特/s。 存储转发传输多数分组交换机在链路的输入端使用存储转发传输机制，它是指在交换机开始发送第一个比特之前，必须接收到整个分组。 我们看一个例子： 如果发送方要传送长是L的报文，两条链路传输速率都是Rbit/s，那么到达路由器之前不能转发，首先浪费了L/R s，这时全部到达了，然后转发出去，一共花费了2L/R。（没有考虑线路长度） 而如果数据一旦到达路由器就转发而不用等到全部到达，只需要L/R的时间。 现在计算发送三个分组，从源发送第一个分组到达目的地接收到全部三个分组的时间：在L/R，第一个分组被转发出去，这时源也开始发出第二个分组，2L/R，第一个已经被路由器转发完毕，目的端收到，路由器开始接收第三个分组，源开始发送第三个分组，3L/R，路由器接收第三个分组完成，第二个分组已经送出去被目的接收到，源端发送完毕，4L/R第三个分组完全传送出去，目的端接收到第三个分组。一共4L/R时间。 考虑N条速率均为R的链路（所以一共有N-1个路由器），发送一个分组所需时间是$t=N*\frac{L}{R}$。 如果像上面一样分组发送： 一个长度L的包一次发出去，时间是$t=N*\frac{L}{R}$，如果分成c组，时间为：$t`=(c+N-1)*\frac{L}{c*R}$！！]]></content>
      <categories>
        <category>计算机网络</category>
        <category>大三的课</category>
      </categories>
      <tags>
        <tag>网络课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络编程一：UDP,CMD,TCP,WebServer]]></title>
    <url>%2F2017%2F11%2F15%2F%E7%AE%80%E5%8D%95udp%E9%80%9A%E4%BF%A1%E5%AE%9E%E7%8E%B0-java%2F</url>
    <content type="text"><![CDATA[UDP CMD今天写webserver的时候发现需要用到以前写的一些程序的知识。所以觉得最好总结一下。 woaibianyi,bianyibangwojiejueledaiamdewenti.xieixeni. 感觉像是…识别不出\n…行吧，myplace myrule，//必须以;结尾！ UDP介绍UDP通信使用两个类：DatagramPacket与DatagramSocket。前者是对UDP包的一个封装，后者是完成两端之间的交流。在UDP里面Server与Client地位同等，彼此没有区分。 1234DatagramPacket packet=new DatagramPacket(buff,buff.length,addr,port);//发送包;DatagramPacket packet=new DatagramPacket(buff,buff.length);//接收包;DatagramSocket socket=new DatagramSocket(port);//指定端口号的socket;DatagramSocket socket=new DatagramSocket();//自动分配一个可用端口号; 不得不说端口号与进程、socket之间的关系：端口号与进程毛关系没有。一个udp socket只能绑一个端口，一个port只能被一个socket绑。这个原因是在多路复用与多路分解讲到的。否则不知道pck究竟该给谁。 ServerServer的ip获取；获取client的ip与portServer端获取Server的ip与port： 12InetAddress.getLocalHost();//ip获取;socket.getLocalPort();//socket绑定的port获取; Server端获取Client的ip与port： 123//pck是从client接收到的包;pck.getAddress();//;pck.getPort();//; 时间获取 定义：Calendar cal; 每次获取时间前都应执行：cal= Calendar.getInstance(); 获取常用的数据： 1234567int year = cal.get(Calendar.YEAR);int month=cal.get(Calendar.MONTH)+1;int day=cal.get(Calendar.DATE);int week = cal.get(Calendar.DAY_OF_WEEK)-1;int hour=cal.get(Calendar.HOUR_OF_DAY);int minute=cal.get(Calendar.MINUTE);int second=cal.get(Calendar.SECOND); 使用其他类访问同一可视化界面以本程序为例，在主类中创建了一个可视化界面，希望在Time类访问可视化界面的一个label： 在Time类创建一个label，使用主类的label初始化这个label。 主方法：监听client请求1234567891011121314151617DatagramSocket socket=new DatagramSocket(4444);//固定的一个socket，绑定固定的port;while(true)&#123; byte[]buff=new byte[256];//输入缓冲区; DatagramPacket pck=new DatagramPacket(buff,buff.length);//每次都新建一个包，否则会产生缓冲区不干净发送数据错误的问题; try &#123; server.log.setText(server.log.getText()+"\ni'm at "+InetAddress.getLocalHost()+" "+socket.getLocalPort()+" and listenning~\n"); server.log.setCaretPosition(server.log.getText().length());//设置目前的光标位置，在可视化界面具体讲; socket.receive(pck);//接收数据包：阻塞方法; server.log.setText(server.log.getText()+"i've get a request from "+pck.getAddress()+" "+pck.getPort()+" "+new String(pck.getData())+"\n");//; server.log.setCaretPosition(server.log.getText().length()); Server newserver=new Server(new String(pck.getData()),pck,server);//新开一个线程，专门负责与这个client的通信; new Thread(newserver).start();//新线程开始运行; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block; e.printStackTrace(); &#125;&#125; pck.getData();:获取pck包的data信息;pck.getAddress();获取包的源地址pck.getPort();获取包的源端口 InetAddress.getLocalHost();:获取本地ip，可能抛出异常，需要处理socket.getLocalPort();获取socket绑定的端口号 socket.receive(pck);:等待接收一个包socket.send(pck);:发送一个包 String与Unix时间戳之间的转换format函数之中是一个Date对象，这个Date对象由一个long构造：list[i]是File类型。也许会说“过时”，没关系，这一整句代码打上去就不会提示了。 1String dates=new java.text.SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new java.util.Date(list[i].lastModified())); 多说一句SimpleDateFormat： G 年代标志符 y 年 M 月 d 日 h 时 在上午或下午 (1~12) H 时 在一天中 (0~23) m 分 s 秒 S 毫秒 E 星期 D 一年中的第几天 F 一月中第几个星期几 w 一年中第几个星期 W 一月中第几个星期 a 上午 / 下午 标记符 k 时 在一天中 (1~24) K 时 在上午或下午 (0~11) z 时区 可以说是十分实用了。 具体信息可以访问这里：SimpleDateFormat使用详解 获取本地文件信息注意：没有解决的一个问题：包最后一定会用ASCII为0的char填充满255个，注意！ 12345File f=new File(dirpath);File[] list=null;if(f.isDirectory())&#123; list=f.listFiles();&#125; 获取文件名信息：list[0].getName();是否文件？是否目录？:list[0].isFile();list[0].isDirectory();还有一系列方法：自己发现吧！ Server的java代码看这里 Clientclient的ip获取InetAddress.getLocalHost();与Server差不多 设置最大时延送出一个包： 123DatagramSocket socket=new DatagramSocket();socket.setSoTimeout(10000);socket.send(pck); 新开一个线程中：等待接收一旦超时触发异常： 12345678try&#123; socket.receive(pck);&#125;catch(SocketTimeoutException e)&#123; e.printStackTrace(); ori=ori+"timeout!:"+socket.getInetAddress()+" for "+pck.getData().toString()+" has no response . try again!\n"; clientthis.tf.setText(ori);&#125; 使用String设置指定IP12345678910111213141516171819202122232425262728293031323334try&#123; int[]ipaddr=new int[4]; String[] ipsplit=new String[4]; data=data.replace('.', '-'); ipsplit=data.split("-");//这两步是当时不懂正则，使的一个小手段; for(int i=0;i&lt;4;i++)&#123; if(ipsplit[i]!=null &amp;&amp; ipsplit[i].isEmpty())&#123; cmd.setText(cmd.getText()+"format of ip addr wrong!split by .\n&gt;"); rows++; break; &#125; ipaddr[i]=Integer.parseInt(ipsplit[i]); &#125;//前面是为了验证给定的string是符合要求的int.int.int.int形式; //得到正确的ip; //转化为byte流; byte[]byteaddr=new byte[4]; byteaddr[0]=(byte)ipaddr[0]; byteaddr[1]=(byte)ipaddr[1]; byteaddr[2]=(byte)ipaddr[2]; byteaddr[3]=(byte)ipaddr[3]; //设置新ip; addr=InetAddress.getByAddress(byteaddr); System.out.println(data); cmd.setText(cmd.getText()+"&gt;");//先忽略就好;&#125;catch(NumberFormatException e)&#123;//根本不符合格式; e.printStackTrace(); cmd.setText(cmd.getText()+" wrong:"+command.substring(7)+"wrong format of ip addr! split by . and only numbers are accept\n&gt;"); rows++;&#125;catch(UnknownHostException he)&#123;//; he.printStackTrace(); cmd.setText(cmd.getText()+"unknown host\n&gt;"); rows++;&#125; client代码看这里 可视化可视化部分也在上面的client代码里面。 TCP WebServer这是第二次作业，使用TCP，利用http1.1，实现一个可以与浏览器进行交互的小型的WebServer。 附加：tcp部分编码设置 String code=”utf-8”; try { InputStream instream= new java.io.FileInputStream(file); byte[] b = new byte[3]; instream.read(b); instream.close(); if (b[0] == -17 &amp;&amp; b[1] == -69 &amp;&amp; b[2] == -65)code=”utf-8”; else code=”GBK”; } catch (IOException e) { // TODO Auto-generated catch block e.printStackTrace(); } return code;]]></content>
      <categories>
        <category>计算机网络</category>
        <category>大三的课</category>
        <category>作业</category>
      </categories>
      <tags>
        <tag>网络课</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OS-Lab3]]></title>
    <url>%2F2017%2F11%2F14%2FOS-Lab3%2F</url>
    <content type="text"><![CDATA[Lab3：User EnvironmentPre：实验准备知识GCC内联汇编特权降级基础知识准备切换到保护模式之后就会有不同的特权等级，这时默认在最高的特权等级上。这个时候为内核建立了页表结构，接下来将会跳转到用户程序。进行跳转的时候，应该注意：不可以再留着这个高级的运行权限了，因此需要进行特权级的降级。 系统在以上电时，各个资源管理都还没有建立，这个时候处于特权管理模式，在特权模式建立完毕所有的资源映射之后，再降级到通用模式，运行。 因此，一上电运行的指令一旦被不安全的程序接管，将不会降级，保护措施将形同虚设。因此，在智能手机等终端上都有一个叫做trustdo(?)的硬件，它是一个加密的模块，这个模块是系统上电之后运行的第一段代码，作用是验证将要运行的这段代码是不是被授权的，数字签名是否通过，若通过才会运行。trustdo会被做到CPU的芯片中，保证了即使上电后处于特权模式的代码是安全可靠的。 iret：系统指令，从中断中返回。ret：从函数中返回retf：与ret基本一样，从函数中返回。（弹出栈，拿出返回地址（就是下一条语句）返回值，丢掉压栈参数） 中断是一个硬件的事件，中断返回与函数返回不同。当一个中断来了，不管你当时在干嘛，就乖乖地保存一下现场去响应中断。这个保存现场，包括了硬件将会自动帮你保存的一些现场，比如CS、SS、Flag等等，这些是自动存下的，iret的作用就是自动地将这些东西都弹出。但是这些现场保存并不够，通用寄存器是不会帮你保存的。 ret、iret等等在汇编之中可以随意调用，并不需要其他约束。如果当你在汇编中不是函数（中断）的里面调用了这些，它会干些什么呢？他会把自己认为的自己保存进去的那些东西弹出来，一个是返回地址给PC，一个是返回值给保存返回值的东西。即使没有调用函数，ret的行为也是固定的：从栈顶弹出两个值，一个给PC，一个给存储返回值的东西，iret的行为也是固定的，就是从栈顶弹出东西给那些变量。这就是保护现场之后的栈。ret是弹出到eip，retf弹出到CS，iret弹出到CS（代码段）、EFLAGS，根据是否改变特权等级，iret会连下面的SS以及esp一起弹出来。 这样，如果一个程序在栈中事先在栈中存了一些数据，然后调用了iret，iret不管栈中究竟是什么，就会弹出5（3）个元素，给对应的寄存器。ret和retf也类似。因此，在汇编中ret们与高级语言的return们的作用不同，汇编中的ret们只是提供了一个批量修改寄存器的方法。 系统中有两种中断。中断可能在任意时间发生。在用户模式下与在特权模式下中断发生后系统的动作是不相同的。 在用户模式下，系统会保存这些东西：还记得在“内存管理”的lab里面讲过的段选择子，其中有两个bit标识自己处在什么特权模式下的特权位。这里面有两个段选择子，一个是栈的，一个是代码段的，由于在用户模式下，可以看到CS代码段选择子与SS栈选择子的特权位RPL都是3。 在特权模式下，一旦发生中断：这时保存的东西比较少。 无论在哪种模式下一旦调用iret，就把这些东西（其实不一定是这些东西，只是对应栈顶的几个数据）弹出去给对应的寄存器。 正题：特权降级考虑做完lab2，系统在特权级是0的模式下，这是需要现将自己降级成为特权3的模式，再去运行用户程序。怎么降？ 要伪造一个场景：刚刚一个用户程序产生了一个中断所以我升级了，现在我要中断返回。于是我去伪造了一个栈，在栈里头存上了iret希望的那5个数据，每一个是32位。然后调用iret，这些就会从栈中弹出来到达对应的寄存器，然后我就回到了用户模式，那两个段选择子的特权级值就被改为了3。这里需要注意的是，系统原来使用的特权级是0的栈，iret之后这个栈空掉了，但是系统不再使用这个栈，而是重新为用户态开避了一个新的栈，这个栈不再是以前的那个栈了，这个栈放在用户空间的数据段里，它的权限也是3。 为什么要这样设计呢？在特权模式下，系统并不想和特权级更低的用户态共用一个特权级更高的栈。另一个方面从安全考虑，为了防止用户态随意弹出数据随意插入数据破坏内核运行。 从特权模式到达用户模式，新创建了一个栈给用户程序使用，这是在刚上电的时候，还没有用户程序运行。用户程序运行过程中可能会有系统调用回到特权模式下，这是将会使用前面提到的系统用的那个栈，系统调用完成之后，不会再为用户程序创建新的栈了，一是占空间，一是以前的运行态不能就这样丢掉。就还让它回到自己原来的栈。 特权级提升中断、异常、系统调用：我需要更高特权的东西帮帮我。 在80386中，系统调用就是一个中断。异常是需要紧急处理的中断。 中断在32位机器上的中断是一个很复杂的机制，因为那时保护模式已经建立了起来，整个运行在虚拟地址空间上。所以当中段发生的时候，系统使用中断描述符表，写着自己所有的中断号。中断描述符寄存器（IDTR）中存的就是中断描述符表（IDT）在内存中的位置以及大小。表按照中断号排序，每一个中断描述符在80386中被叫做一个门，中断门（中断），陷阱门（异常）。会根z中断发生的编号从表中取出对应的描述符（门），取回的描述符是这样的：segment selector与offset用来描述在段机制下自己的中断响应函数的位置。segment selector负责在GDT里面找到对应的段，offset负责找对应的偏移量。找到函数之后，保存现场并执行响应函数。 在终端发生的时候，系统将会看一下产生中断的程序的特权级，决定应该怎样保护现场。如果在特权状态，不用换栈，只需要保存更少的东西，而处于用户状态下的程序在执行完中断响应函数之后还要从（？）退回到自己的用户栈，保存了更多的东西。 正题：特权级提升从特权模式3切换到特权模式0，实际上就是响应一个中断，这就是上面图中的trap。 对于用户程序，os并不信任。当trap发生时，当前用户程序的运行状态会被存入到内核栈中，而用户栈并没有变化。这也是中断与普通函数调用的不同。现在已经进入了特权模式，如果不作任何操作，只是调用一个iret，前面讲到的CS，SS特权级3又会被写入到寄存器里面，就又回到了用户态。为了能留在特权模式下，对栈进行伪造，假装在进入终端之前就是一个特权模式：也就是把上面提到的用户态发生中断的栈信息修改成特权模式发生中断的栈信息。这时iret，就可以留在特权模式下。 但是：在伪造的时候，SS被扔掉了。怎么回去呢？80386提供了一个TSS任务状态段，它几乎可以存下CPU上所有的寄存器的不止一份拷贝。TSS每个进程一个，会存下一个进程在切换特权状态的时候原特权状态下的SS的值。系统将会自己对TSS进行维护，以确保切换模式时栈的跳转。 TSS存在于哪里呢？在GDT里面有一个TSS的描述符： 80386中有4个特权模式，目前在虚拟化技术下只是用了3个。 有一个叫做TSR的寄存器，存着selector、base addr以及segment limit。便于直接找到TSS。 如果想要回到用户模式，进入trap，伪造一个从用户态过来的栈，iret即可。 系统调用用户程序可以通过系统调用访问内核服务，这个过程需要指定中断号，使用tarp或者特殊指令（SYSENTER/SYSEXIT）实现。 关于这次实验这里的env就是课上讲的pcb。父子进程的一个作用是父进程要为子进程收尸。 elf格式：linux下的可执行。创建进程并执行elf：没有文件系统怎么办？把指定的文件链接进目标文件，骗bootloader把程序也搬进内存。 -b binary path:-b帮我搬进来。binary这是个可执行程序。然后重生成一个符号表。把这个文件作为一个大数组缀到最后，然后直接使用这个数组。 编码的时候，这个数组还没有，链接的时候才有，怎么办？定义一个外部变量extern。只需知道生成符号表的规则，然后按规则命名即可。 进来之后还只是一个大数组，还需要经过链接之后映射要对应的虚拟地址才能执行。]]></content>
      <categories>
        <category>OS</category>
        <category>MIT Lab</category>
      </categories>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F11%2F13%2FOS4%2BMemory%20and%20Process%2F</url>
    <content type="text"><![CDATA[系统调用例：STL的实现printf：对屏幕： 在每一个像素有一种颜色 文本模式 系统调用与函数调用系统调用是中断，相当于在用户程序执行的过程中嵌入一段由OS完成的代码，完成后再返回用户程序。其中，包括特权级的升降。在进行系统调用的过程中，进入之后首先会检查用户传参是否有危险，随后再执行系统调用。这将在一个新的页完成，存在于内核态独立地址空间，将会带来包括TLB，Cache等的一系列问题。因此，系统调用的代价比函数调用的代价大得多。 进程间的通信直接通信与间接通信1send(A,) 阻塞通信与非阻塞通信像网络中那样，阻塞通信就是// 通信链路缓冲0容量：发送方需要等待接收方有限容量：无限容量： 信号信号类似于中段， 是最原始、最简单的一个进程间交互的模式。在关机时，系统将会用这种方式通知各各进程。 管道父子之间传递信息 消息队列类似于电子邮件系统。 得到一个队列，使用队列号来标识。此后消息的send与recv都通过这个队列。 共享内存创建共享段shmget(key,size,flags)。其中key是其他人希望与你共享内存的话必须知道的值。shmat(shmid,*shmaddr,flags)把共享段映射到进程地址空间。 在这里，将又会遇到在“哲学家就餐”那部分的问题。另外，还将会有在cache还是内存的问题。cache作为在读写操作时第一个碰到的硬件，是否可以保证进程A写的时候吧东西写进了共享区的物理内存？在B不能使用的时候它的Cache的正确性哈能不能保证？—？ 由一个“可否使用cache”的位]]></content>
  </entry>
  <entry>
    <title><![CDATA[哈希]]></title>
    <url>%2F2017%2F11%2F12%2F%E5%93%88%E5%B8%8C%2F</url>
    <content type="text"><![CDATA[emmm…大二的时候DS只考了80，出来混的，总是要还的。 不希望文件太多都小于512k，会占我的空间。哈希哈希其实没什么神秘的，不过是一群贪心的人的产物。 散列函数，将key转化为对应的value值得到索引。这个有诸如取模，ASCII平均，等等方法。橙皮书上有一种sfold，似乎性能会更好一些。 开哈希开哈希类似于静态链表，每一个value对应一个链表，成为挂着的桶。 闭哈希闭哈希将node存储在给定的一个数组中。这个数组的下标将作为value的索引值，由于value可能重复，需要探查方式，有很多方式，比如线性探查与二次探查。 关于闭哈希的删除，其实不必真的每次都向前挪，而是增加一个“墓碑”，它代表这里没有数据可以被插入，但是又不是真的空因为需要继续向前探查。当次数多了性能就会下降，一种解决方式是向前挪，另一种是重新散列。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语言们的输出格式控制]]></title>
    <url>%2F2017%2F11%2F12%2F%E8%AF%AD%E8%A8%80%E4%BB%AC%E7%9A%84%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[待整理]]></content>
      <categories>
        <category>语言</category>
      </categories>
      <tags>
        <tag>语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理——词法分析]]></title>
    <url>%2F2017%2F11%2F09%2F%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E2%80%94%E2%80%94%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[我有罪….我先是用学编译的时间逛了淘宝….然后又用学编译的时间逛了六维…..我有罪….\infty 时间：1012cp3 3:15开始 词法分析器介绍 编译并不是一件轻松的事情，对于较大的系统，编译的速度并不理想，这也是为什么我们需要优化编译技术。 基本概念词法单元（单词）：由一个词法单元名以及一个可选的属性值组成。名即是语法分析器的输入符号token。模式：一个词法单元的词素可能具有的形式。一个词素不能与两个或多个词法单元进行匹配（二义性）。词素：源程序中的字符序列，是程序中实际出现的字符串。 在lex与yacc中，有时会有一个全局变量。他保存了当前词素（词法单元：关于这里，龙书里面的和wg写的不一样呢？）的属性值，这个属性值可以被记录在语法树上。 词法分析器要干些什么？ 读入源程序字符，生成词素，确定词法单元序列 与符号表进行交互 过滤掉源程序中的注释与空白 将编译器生成的错误信息与位置联系 预处理：扫描阶段完成不需要生成词法单元的简单处理；词法分析阶段处理扫描阶段的输出并生成词法单元 跳过了一些东西 正则表达式（正规表达式、正规式）为什么需要正则表达式来描述字符串序列呢？像在上下文无关文法中所描述的那样，因为字符们所组成的可能的字符串是无穷的。 单词是什么？单词的本质就是符号串的集合。正则表达式就是代替了自然语言对特定符号串集合的描述。 概念符号表：符号的有穷集合符号串：字母表中符号构成的有穷序列。也成句子，字。|s|代表字符串s的长度，$\epsilon$是空字符串。语言：是一个给定符号表上的特定的符号串的集合。 比如：给定符号表{1,0}，有符号串01100010，一种语言是{0，1，00，11，000，111….}即串中只含一种字符的字符串集合。 特例：$\varnothing$是空语言。{$\epsilon$}是只含空串的语言。 运算符号串的运算连接：x=hou y=se xy=house s$\epsilon$=$\epsilon$s=s (emmmm说好的不能这样写呢？)幂：$s^n=s^(n-1) * s^1$ 语言的运算下表中运算符的优先级由低到高 运算 定义 并 $L\cup M={ s \ s\in L 或 s\in M} $ 交 $LM={ st \ s\in L 且 t\in M} $ Kleen闭包 $L^*=\cup_{i=0}^\infty L^i$ 正则闭包 $L^*=\cup_{i=1}^\infty L^i$ $$\color{red}{正则表达式&lt;-&gt;语言运算的简洁描述}$$ 正则表达式定义字母表$\sum$上的正规式r的定义规则，以及r所表示语言L(r)定义： $\epsilon$是正规式，表示语言{$\epsilon$} 若$a\in \sum$则a是正规式，表示语言{a} r , s是正规式，表示语言L(r)与L(s)，则： (r)|(s)是正规式，表示语言L(r)$\cup$L(s) (r)(s)是正规式，表示语言L(r)L(s) $(r)^*$是正规式，表示语言$(L(r))^*$ (r)是正规式，表示语言L(r) 第三条的四条，优先级从上到下依次升高。 $(a|b)^*$={所有由a、b组成的符号串} 正规式等价：r=s &lt;-&gt; L(r)=L(s) 正则运算的特性: 可以像产生式那样，为正规式指定名字： num -&gt; r1其实，上下文无关文法的描述能力包含了正则表达式的描述能力。所有可以被正则表达式描述的都可以被上下文无关文法描述，但是正则表达式并不能描述某些上下文无关文法可以描述的东西。那么为什么需要正则表达式来实现词法分析器？ 简化编译器的设计：使每一阶段需要做的事情更加简单清晰 提高编译器的效率：使用专门的字符缓冲技术提高编译速度 增强编译器的可移植性：输入设备相关的特殊性被限制在词法阶段 下面举一个例子： 无符号整数：digit -&gt; 0|1|2|…|9digits -&gt; digit digit* 其实也可以是$digit^+$optional_fraction -&gt; .digits | $\epsilon$optional_exponent -&gt; (E(+|-|$\epsilon$)digits)|$\epsilonnum -&gt; digits optional_fraction optional_exponent 符号简写： +:一个或多个实例?:0或1个实例：$r?=r|\epsilon$$\rightarrow$$L(r)\cup {\epsilon}$[]字符集:[abc]=&gt;a|b|c、[a-z0-9]=&gt;a|b|..|z|0|1..|9 非正规集正规式无法描述的语言：{wcw|w是a、b组成的字符串}正规式无法描述平衡或嵌套的结构正规式只能表示：有限的重复、一个给定结构的无限重复 关于正规式的练习，参考第三章练习 Lex使用流程与yacc类似。 $\color{red}{ATTENTION! 在使用project wizard建立lex的时候你写的那个词法分析器的名字就是那个名字， 你最好别自己再修改，否则在VS里面运行不出来！！！}$ 给的是字符流，出来的是单词流。 规则段放正则表达式与语义动作。 第四次作业词法分析器设计将会上传。click here to get 有限自动机有限自动机可以直接转换成程序。 NFA从正则表达式到自动机从这之后，正是lex所做的事情。 在这一节里，将介绍两个过程：一个是正则表达式到NFA，另一个是NFA到DFA。之所以让NFA做一个过渡，是因为正则表达式直接到DFA的算法很复杂。本次更新掠过。 NFA与DFA的性能差别很大：NFA占用空间比较少，但是使用NFA进行词法分析需要在错误态停止，可能需要花费$O(2^n)$的时间复杂度；DFA占用空间很大（最坏情况下$O(2^n)$），但是使用它进行词法分析的时候接近线性时间。 正则 -&gt; NFA正则表达式构造NFA使用MacMaughton-Yamada-Thompson算法。简称Thompson算法。这个算法描述如下： 基本规则： 自动机运转 s $\leftarrow$ e -closure({s0});c $\leftarrow$ nextchar;while c $\neq$ eof do s $\leftarrow$ e -closure(d(s,c)); c $\leftarrow$ nextchar;end;if S$\cap$F$\neq$ $\varnothing$ then return “yes” else return “no” s $\leftarrow$ s0;c $\leftarrow$ nextchar;while c $\neq$ eof do s $\leftarrow$ d(s,c); c $\leftarrow$ nextchar;end;if s is in F then return “yes” else return “no” 前面提到了，NFA占空间更少，DFA识别字符串更快，贪心的人类啊希望能糅合这两者的优点。看前面的NFA代码，其实在第四行那句，实际上就是前面提到的构造DFA的过程。人们想到使用Cache。使用NFA，当NFA构造出DFA的一个状态时，就把这个状态构造的条件以及状态本身存到cache里面。当while进一个c的时候，首先看一看现今状态加上c到达的态是不是已经存在cache，如果存在里面，最耗时间的那一部分就不用运行了。cache的管理仍然是程序局限性原则。占用空间不会太大以期望达到折衷的效果。 如果要构造出一个Lax，需要将所有的正则表达式得到的NFA进行一个并操作（保留各自的终态以区分）。然后构造它的DFA。 值得注意的是，这样不能在一个终态停止，而是在错误态终止并退回到最近经过的那个终态（每经过一个终态，记录当前的输入指针以及匹配模式）。以防止找到前缀，产生错误。这种方法是最长前缀法，在前面已经提到过。 DFA优化DFA状态数其实不会太多的影响时间复杂度，当状态变少的时候，将会减少存储空间的消耗。 区分：一个符号串可区分两个状态：这一个串从这两个状态出发在DFA上得到的结果一个是accept，一个是reject。在做区分的时候，其实可以看到：最终字符串得到的终态在不同集合，则他们是可区分的。 Exa不能区分状态A与B。而且还可以看到，以a开头的所有字符串都不可能区分A与B了。再进一步看到，a不可能区分任意两个，因为所有状态在经过a的状态迁移之后都到达了状态B。同理，b不可能区分ACE，以b开头的都不能区分ACE。但是b却可以区分BD。不要忘记，$\epsilon$可是可以区分的。而且他的区分应该被放在最前面，那是因为$\epsilon$区分了终态与其他状态。 在区分的过程中发现了这样一个规律：如果对于串s，A状态与B状态经过了s都到达同一个状态C，那么以s做前缀的字符串都不可能区分A与B了。当所有字符串都不能区分A与B的时候，可以把A与B合并成同一个状态。但是对于无穷的字符串，怎么确定 “所有都不行” 呢？ 在前面，我们已经提出了一个解决方案——当前缀s不行的时候，所有的sx都不行了。但是，即使是有了这个规律，使用“不行”的这种方法解决问题仍会显得繁琐。 真正在执行的时候，实行的是分裂，这样，通过有顺序地枚举字符串，将能被区分开的状态分开。这样对于前面的不同集合的说法，也能更好地理解。 算法描述：首先使用$\epsilon$区分终态与非终态，将它们分为两个集合。随后，像子集构造法那样，有顺序地枚举输入字符串，将那些经过状态迁移能到达终态（不同集合）的状态剔除出去组成一个新的集合（如果两者到达同一个集合，这两者是不能区分的，应该在同一个集合），对于那些含有多个元素的集合，如果经过一系列不同的状态迁移到达不同的集合，仍需要继续分开。直到不再产生新的集合。 仍然使用上面的例子，进行分裂的过程： $\epsilon$ -&gt; {A,B,C,D}、{E} a -&gt; {A,B,C,D} ({B,B,B,B})-&gt;全部一样，以a打头的不用再试 b -&gt; {A,B,C}{D} ({C,D,C,E}) b -&gt; {A,C}{B}{D} ({C,C,D}) -&gt; 不同集合 最终得到的最小DFA： 最后使用新的状态进行迁移的时候，一定不会出现问题。 补充：从自动机到正则表达式可以从一个DFA或NFA得到对应的正则表达式。但是这种方式并不是总是简单的，他只是提供了一个模糊的思路，当一个DFA很复杂的时候，还是需要有聪明才智才行。这里只是简单的给出这个方法，具体可查看这个文件。 为了使讨论更加简单，我们强制一个DFA/NFA应该有下列特性： 初态可以到达任意其他状态，没有状态可以通过状态迁移到达初态 只有一个终态，终态不能通过状态迁移到达非终态 初态不能是终态 除了初态与终态，其他状态彼此相连 这四个条件看起来很可怕，其实当我们引入了$\varnothing$并根据情况添加初态终态之后，很容易可以把一个DFA/NFA转为一个符合要求的有限自动机。 以一个例子来说明： 为了不让自己沾沾自喜，必须指出上面给出的例子很简单，看下面这个： 由于生成initstate的一个要求是，不能有进入initstate的箭头，好，像上面一样加一个init，加一个Ac，随即，在删除A的时候就会发现问题。这尼玛转来转去的究竟怎么写边上的正则表达式啊？！ 能不能与好不好在龙书的第九章。 流程可否更加简洁？ 正则 -&gt; DFA：本质上还是做子集构造法-吧正则的某些位置对应NFA里面状态，位置集对应NFA的状态集 优化结果？ 最小DFA]]></content>
      <categories>
        <category>编译原理</category>
      </categories>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OS:内存管理]]></title>
    <url>%2F2017%2F11%2F06%2FOS-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[#]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>OS</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程与线程]]></title>
    <url>%2F2017%2F11%2F06%2F%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[进程与线程进程模型的进化： 第一代：relay , vacuum tubes and plugboards。没有程序与进程的概念 第二代：批处理系统：一个程序完成所有工作。 带三代：mutiprogramming and timesharing。CPU可以切换运行。 第四代：现代OS：进程模型更加成熟，进程调度与相互交流更加成熟，内存保护与管理更加成熟。 进程：计算机上所有可以运行的软件，通常也包括操作系统，被组织成若干顺序进程，简称进程。它包括输入输出程序与状态。 需要区分进程与程序。 进程创建：系统初始化、运行进程的创建进程系统调用被执行、用户请求、批处理作业初始化。 进程终止：正常退出、出错退出（自愿）、严重错误、被杀死（非自愿） 在unix中，每个进程有自己的父进程，有着明显的层次结构，但是在Windows中，虽然也有“句柄”来标识父进程，拥有句柄即可控制及对应的子进程，但是这个“句柄”可以被转让，就不存在明显的层次关系了。 为什么要并发（Cocurrency）？进程的状态进程的基本状态有运行态（Running）、就绪态（Ready）、阻塞态（Blocked）。其他运行态还有创建（New）、挂起（Suspended）、退出（Exit）。 状态 说明 运行 目前获得了CPU使用权 准备 可运行，但由于CPU被其他占用而暂时停止 阻塞 除非某种外部事件发生，否则不可运行 – – 创建 进程的数据结构已经准备好，但是running image还没准备好 退出 进程已经完成了自己的工作，但是数据结构还没有删除 挂起 running image被交换到了硬盘 阻塞是由于他自己的原因需要等待输入输出等的操作，逻辑上不能继续运行；而准备是系统把他调度下来了，即使它本身还可以继续运行。 观察上面的状态迁移图发现：阻塞态是不能直接进入运行态的，必须经过准备状态。同样，在准备状态的进程根本没有得到CPU资源，也不存在执行到需要外部事件的程度，因此也不可能有向阻塞的转变。 如果加上另外两个状态： 对于挂起状态：有时需要对进程做分级处理，引入优先级会使某进程等待时间过长而被换至外存，这被称为进程挂起，目的是：提高处理机效率：就绪进程表为空时，要提交新进程，以提高处理机效率；为运行进程提供足够内存：资源紧张时，暂停某些进程，如CPU繁忙（或实时任务执行）时内存会比较紧张；便于调试：在调试时，挂起被调试进程对其地址空间进行读写。 双挂起： 在这个图中，挂起状态下对应进程的数据都在外存中，而其他状态下对应的数据应该在内存中。从进程一旦被调入外存，必须在所有都准备好之后才可能被从外存调入。等待外部事件发生的进程在外存中等待完之后才可能有机会进入就绪状态准备运行。 相对于内存，外存的速度很慢，什么导致了进程数据被移到外存？一是就绪进程需要更多的内存资源，只能把那些现在还不可能就绪运行的进程移出去，这是进入阻塞挂起状态，二是更高优先级的阻塞（阻塞挂起）进程终于等到了自己的外部事件，要求先运行，所以较低优先级的进程就被移入了外存，这是进入就绪挂起状态。对抢先式分时系统，当有高优先级等待挂起进程因事件出现而进入就绪挂起（运行-&gt;就绪挂起） 什么时候把外存的数据转移到内存呢？一是没有就绪进程或挂起就绪进程优先级高于就绪进程，这时进入就绪状态；二是一个进程释放足够内存，并有高优先级等待挂起进程，这是进入阻塞状态。 在不同的OS中，进程状态差异较大！！ Linux中的进程状态windows中的进程状态（XP）与上述双挂起状态迁移类似，下面讨论Linux中的进程状态。 状态 说明 TASK_RUNNING 进程正占用CPU或是等待被执行 TASK_INTERRUPTIBLE 进程被阻塞，直到外部事件发生。发起中断系统资源释放或某种信号都能唤醒进程为RUNNING TASK_UNITERRUPTIBLE 除了传递一个信号给睡眠进程保持其状态不变 TASK_STOPPED 进程停止运行，在接收到SIGSTOP、SIGTSTP、SIGTTIN、SIGTTOU后 TASK_TRACED 所谓running image 进程调度]]></content>
      <categories>
        <category>OS</category>
      </categories>
      <tags>
        <tag>OS</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JUnit之一]]></title>
    <url>%2F2017%2F11%2F05%2FJUnit%E4%B9%8B%E4%B8%80%2F</url>
    <content type="text"><![CDATA[这是一个简单的JUnit入门介绍。 junit是一个测试框架，把一个java工程按照JUnit运行的时候会启动这些检测，但是这并不影响这个项目作为一个JAVA程序的本质。这个程序仍然可以以Java Application来运行。 在eclipse中使用Junit4进行单元测试安装eclipse的时候，应该已经有了junit的jar包，如果没有，可以自己去maven仓库搜索下载。 在eclipse中使用Junit4进行单元测试 为了方便，我会把上述的方法自己实践并写在下面： 不要直接finish。点击next，你将可以选择对哪些方法进行测试，eclipse就会自动帮你命名（which is very essential） 看！]]></content>
      <categories>
        <category>计算机网络</category>
        <category>实验室</category>
      </categories>
      <tags>
        <tag>实验室</tag>
        <tag>计算机网络</tag>
        <tag>模拟器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络应用层之番外二]]></title>
    <url>%2F2017%2F11%2F04%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E5%B1%82%E7%95%AA%E5%A4%96%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[前言其实这里该是运输层番外….但是这是应用层作业啦。 多用户并发访问与可靠文件传输，不能有下载大小的限制。 为了用UDP实现多用户并发访问，一定是需要使用多个socket。仿照FTP的协议设计：使用一个UDP的socket在master进行监听，这个socket绑定一个确定的端口号，用户客户机连接服务器发请求使用。 考虑的请求比较小，可以使用一个包传送完成： Server在端口号X进行监听。 将有一个N。一个client要求连接，发送请求之后立即fork一个子进程，仍然使用这个socket，立即进入准备接收的状态。Server收到了，立即fork一个子线程，master继续监听，确保其他client的请求能被接收到。子线程新建一个UDP的socket，准备进行接收。由于UDP的不可靠，希望在传输数据的时候，Server应该首先，在fork出子线程之后立即对命令请求进行分析，告诉client一共要穿多少个包。（因为在n结束之后，client的子线程就会结束。如果不采用n结束子线程就结束，肯定需要有一个例如，里最后一个包recv之后多久end。这样应该是不太可靠的…对于server，需要有超时重传。emmmm 其实这样也行： client子线程立即等待server传回的数据。同时启动一个定时器，如果超过了一定时间，反正也是同一个socket，就再重传，然后去等。 server呢，在子线程fork出之后就去分析命令，然后开始传包。每一个包有一个序列号。采用选择重传的机制进行文件信息的传输。当发送端的所有发出的包已经发送完毕并且所有的一发送包都得到回执确认之后，向接收端发送最终的发送完毕包。这个包在发送完之后，client接受到之后必须给ACK。server收到ACK之后才能关闭。 有状况：如果client没接到信息或者信息错了，按照选择重传，不会有动作，server重传没毛病；如果client给的ACK丢了，server不能确定client已经收到，也会重传，所以client不可能在发送完ACK就关闭，而是影该等待server的回执，让server知道我已经知道我拿到完整的包了。一旦等到，立即关闭。所以client这边对于最后一个包的动作是：recv-&gt;收到错报，不动，等重传/收到对的包，回复ACK同时启动一个超时重传机制，开始接收server的回执，不管接受到了什么，关闭。（因为这个时候server只要收到了client的东西就证明client肯定已经知道包已经传完了）因为server最终只发送一次，如果client连着很多次超时重传都没有收到回应，很大可能已经server关闭了，为了不陷入无休止的重传，client关闭。 server的动作是-&gt;发送最后一个结束包-&gt;超时重传/接收到client的信息-&gt;发送一个ACK，关闭。 server也知道client这时server也已经关闭 最终交付 计算机网络书面作业：网络协议设计 yayi2456 &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;本协议仿照了FTP协议以允许多客户机的并行操作，使用选择重传的方法在UDP的基础上进行一定程度的可靠性的保证与效率的保证，加入其他机制保证大文件的可靠传输。 &nbsp;&nbsp;&nbsp;&nbsp;server端的主socket绑定一个端口号，进行对客户请求的监听。一旦监听到客户机的请求，立即fork一个子线程，在子线程中重新绑定一个socket，由这个socket与客户机进行通信。 &nbsp;&nbsp;&nbsp;&nbsp;client端根据程序并行度可选择是否需要在子线程发出请求，这取决于不同的应用程序的动作，不做讨论。提到client的动作时，并不指定是在哪个进程。 &nbsp;&nbsp;&nbsp;&nbsp;将该协议描述如下： &nbsp;&nbsp;&nbsp;&nbsp;server监听端口A，一旦监听到client的请求，立即fork子线程。在子线程中对该请求进行分析，并决定需要发送给client的数据。 &nbsp;&nbsp;&nbsp;&nbsp;client向server发送请求，并启动一个定时器，当超过一定时间仍未收到server传回的第一个包，client重新发送请求信息。使recvbase为0，收到第一个数据包之后，recv=(recv+1)%(2*N)，该定时器关闭。 &nbsp;&nbsp;&nbsp;&nbsp;注：在server收到请求之后，并没有立即发送ACK通知client自己已经收到了请求。原因在于server最终一定会发送数据给client，只需要把第一个数据包作为server收到的确认信息即可，同时也减少了可靠性保证所花费的额外的时间。但是由于server对准备发送的数据需要做一定的处理，这个时延可能会稍长。 &nbsp;&nbsp;&nbsp;&nbsp;server开始向client发送包。采用选择重传机制，server为每一个包附上一个序号，同时利用UDP头本身具有的checksum字段进行一定程度的检错判断。server每次最多传送N个报文段，协议中具有2*N个序号。初始时，发送缓冲区中的sendbase是0，server每传送一个报文段就会启动一个对应于该报文段的定时器，当某个报文段的定时器超时之后，server对该报文段进行重传；当server收到了某个序号的报文段的ACK之后，将该序号置为“已确认”，如果该序号是sendbase，那么将sendbase=(sendbase+1)%(2*N)，如果sendbase是“已确认”，再将sendbase=(sendbase+1)%(2*N)，直到sendbase是“已发送但未确认”或是“可用”。同时，当sendbase改变时，如果包尚未发送完，则可以使用新的“可用”来发送新的报文段。 &nbsp;&nbsp;&nbsp;&nbsp;client接受到第一个数据包之后关闭第一个定时器，并向服务器发送对第一个数据包对应的序号的ACK，随后开始接收其他数据包，每接收到一个检验和没有错的数据包，返回一个对应该数据包序号的ACK包。无论是不是server端没有接收到上一个对应该序号的ACK，返回一个ACK，对于最大为2*N的空间来说，是没有问题的。对于在[recvbase,recvbase+N-1]空间中的序号对应的包加以解析送给应用层使用或者是缓存之后将对应的序号标为“接收已确认”，而另外的序号对应的数据包只返回ACK，对数据包直接丢弃。每次接收到一个被“接收已确认”的数据包，如果该数据包的序号是recvbase，recv=(recv+1)%(2*N)，如果这时recvbase对应序号仍是“接收已确认”，recv=(recv+1)%(2*N)，直到recvbase对应序号是“期待但未收到”或是“可用”。 &nbsp;&nbsp;&nbsp;&nbsp;在所有的有用数据包传送完成之后，server需要告知client，有用的数据已经传送完了。 &nbsp;&nbsp;&nbsp;&nbsp;场景分析如下： &nbsp;&nbsp;&nbsp;&nbsp;当server的所有发出的包已经发送完毕并且所有的已发送包都得到回执ACK之后，向client发送最终的发送完毕包。这个包在发送完之后，client接收到之后必须给ACK。server收到ACK之后才能关闭，否则可能导致client不知道自己已经拿到完整的数据而陷入空转。 &nbsp;&nbsp;&nbsp;&nbsp;如果client没接到最终包信息或者信息错了，按照选择重传，client不会有动作，server等待超时之后重传；如果client回复的ACK丢了，server不能确定client已经收到，必须重传来确认client已经知道自己可以关闭连接了，所以client不可能在发送完ACK就关闭，而是该等待server的回执，让server知道client已经知道client拿到完整的包了，否则可能会使server陷入无尽的重传。一旦等到server的回执，client立即关闭。若许久都未等到回执，关闭。 &nbsp;&nbsp;&nbsp;&nbsp;下面对最后的动作进行描述： &nbsp;&nbsp;&nbsp;&nbsp;server的动作是：发送最后一个结束包，启动定时器，若超时则进行重传；直到接收到client的信息之后，关闭定时器，server发送一个ACK，立即关闭。 &nbsp;&nbsp;&nbsp;&nbsp;client最终的动作是：等待接收数据，如果收到错误的包，等重传；如果收到对的包，回复ACK同时启动一个定时器，启动超时重传机制，并开始接收server的回执，不管接受到了什么，关闭。若超时一定次数之后仍未接收到server的回执，因为server最终只发送一次，如果client连着很多次超时重传都没有收到回应，很大可能已经server关闭了，为了不陷入无休止的重传，client关闭。 从宏观的角度来看，客户机与服务器的交互是： 扩展FSM： server： client：]]></content>
      <categories>
        <category>计算机网络</category>
        <category>大三的课</category>
        <category>作业</category>
      </categories>
      <tags>
        <tag>网络课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络番外三]]></title>
    <url>%2F2017%2F11%2F04%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%95%AA%E5%A4%963%2F</url>
    <content type="text"><![CDATA[第二次书面作业 在目前的报文交换网络中（如Internet），主要采用存贮转发式交换。源主机通常将应用层较长的消息（例如，图像、视频等）分成小的报文段在网络中进行传输，接收端再将报文段组合成原始的消息，提交给应用层。下面我们给出了消息直接传输（不分报文段）和分成报文段传输的示意图，假设消息长度为8×106 bits，每条链路的传输速率为2Mbps，忽略传播延时、排队延时和处理时间。请回答下列问题： 1) 如图a所示，如果消息不进行分段直接进行传输，每台交换设备均采取存储转发式交换，请计算消息从源主机发出到目的主机完全接收所需的时间； 答：$t=3*8Mb/2Mbps=12s$ 2) 如图b所示，如果消息被分成800个报文段进行传输（忽略各层的封装），每个报文段长10000 bits，请计算消息从源主机发出到目的主机完全接收所需的时间； 答：$t=3*10000/2M+799*10000/2M=802*0.01M/2M=4.01s$ 3) 比较消息交换和报文交换的优缺点，除了传输延时方面的考虑，采用报文交换还有哪些其他方面的考虑？ 答：消息交换省去了对分片数据进行处理合并的时间、减少了首部的数据传输，但是时延长，可能会长时间占用路由器的大量缓存空间。报文交换大大减少了时延，同时由于数据的减少检错也变得更加容易，错误发生率减小，可靠性提升，但是需要额外传送首部数据，并且需要注意报文的顺序合并，还会带来排队时延。采用报文交换除了对时延方面的考虑，还有对发送优先级以及数据可靠性的考虑。另外，过长的消息堵塞路由器可能导致其他消息不能及时送往目的地而带来一系列问题。 发送者A和接收者B之间使用TCP协议进行通信（A发送数据，B回送ACK）。假设TCP连接建立之后A立即开始发送数据（第一个数据段随三次握手中的最后一个ACK一同发送，初始序列号为1）。链路带宽（传输速率）为100 Mbps，往返延迟RTT为10ms，MSS为1000字节，最初的拥塞窗口设成1个MSS，假设接收端有足够大的缓存空间，拥塞控制的初始阈值设为64。试回答下列问题：1) 假设A缓冲区中有7000字节数据要向B发送，发送的每个数据段均包含1000字节数据，请画出A、B之间的交互过程，并计算所需的时间（从发起连接开始计算，要求给出计算过程）。 从图中可以看出：一共经过了4次RTT以及3段将数据报送出的时间，加上TCP/IP首部字节数认为是40，因此：时间：$t=4*RTT+3*((40+1000)*8/100M)=40+0.2496ms=40.2496ms$ 2) 快速重传机制是对TCP性能的优化，考虑第一问中的传输情况，如果传输过程中有数据段丢失，那么第几个数据段的丢失有可能触发A的快速重传？解释原因。 答：承载数据的第四（seq=3002）个：如果第一个数据包发生丢失，B不会发送ACK，将会因此超时引发慢启动；如果第二个丢失，只会发送一个冗余ACK，然后超时进入慢启动。如果第三个丢失，接下来可以连续发送2个报文段，由于第三个包无法被确认，只能回收两个冗余ACK。如果第四个丢失，5、6、7都正确到达，将会引起3个冗余ACK，会引发快速重传。五、六、七丢失都不能引发快速重传了。 3) 假设发送端发送一系列数据段（1、2、3……n），但A一直未收到任何确认（ACK），正常情况下，第一个数据段的重传定时器会首先超时，A将TCP的拥塞窗口设置成1个MSS，并重传第一个数据段。如果我们现在修改TCP协议，在上述情况下不重传第一个数据段，而改为发送第n+1个数据段，请你分析在什么情况下这种做法有利，在什么情况下不利。 答：如果接收方其实全部收到了包，只是发送方的定时器时间设置太短或是突然拥挤的网络导致包传输减缓，这时发送第n+1个数据段是有利的。因为这时前面数据段的ACK会相继到达，n+1包是最终一定会发送出去的包，因此所有工作都不是无用功；如果重传1包，那么在1包原本的ACK到达之后，这个被重传的包就是冗余包，将会被丢弃还会浪费资源。在其他情况下，比如有部分包甚至全部包没有收到，这种机制似乎并不能有效地解决问题：比如有一个包x（0&lt;=x&lt;=n）未收到，这时无论重传多少次n+1包，最终可能收到的都是ACK=x，陷入无尽的循环。 第三次书面作业]]></content>
      <categories>
        <category>计算机网络</category>
        <category>大三的课</category>
        <category>作业</category>
      </categories>
      <tags>
        <tag>网络课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[覆千秋1]]></title>
    <url>%2F2017%2F11%2F03%2F%E8%A6%86%E5%8D%83%E7%A7%8B1%2F</url>
    <content type="text"><![CDATA[夜色凉将军府的海棠开了。 粉粉嫩嫩，一簇一簇的，细风吹来，花朵轻轻落在地上，把这个小园衬出几分诗意。 只是府上光景却不如这海棠繁盛。 前日里前线传回来消息，说是这宅院的主人被俘了。被俘了，是不要紧，要紧的是他帮着敌军截断了己方粮草，杀了一名副将。 皇帝知道之后龙颜大怒，直接下令封了将军府。管你是真降还是假意。 大臣们都以为，功高盖主，老将军即使不降，回来也风光不了几日了，没差。 平民慷慨激昂，市面上的书里，茶馆里，甚至风月里都是对老将军叛国的不耻，这些模糊的面孔在舌灿莲花的时候，选择性的忘记了老将军当年战绩的辉煌。 当街月色应如水“千霜——”，打扮精致的女人独自从廊中走来，在几步之外站定，唤。 坐在小亭中的女孩循着声音忽地回头，抬着脸叫：“娘——”。]]></content>
      <categories>
        <category>我的故事</category>
        <category>枯骨流沙</category>
        <category>覆千秋</category>
      </categories>
      <tags>
        <tag>千霜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络网络层]]></title>
    <url>%2F2017%2F11%2F02%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%BD%91%E7%BB%9C%E5%B1%82%2F</url>
    <content type="text"><![CDATA[#]]></content>
      <categories>
        <category>计算机网络</category>
        <category>大三的课</category>
      </categories>
      <tags>
        <tag>网络课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络运输层]]></title>
    <url>%2F2017%2F11%2F02%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%BF%90%E8%BE%93%E5%B1%82%2F</url>
    <content type="text"><![CDATA[这些东西是很简单，可是想来想去还是决定记录一下，以免以后遇到像C++语法不知道一样的尴尬。 概述与运输层服务运输层的协议提供的是逻辑通信，并不是直接相连的。 现在我们聊的是对等层通信，本机的运输层发数据给其他机器的运输层。运输层协议在端系统里面，并不是在路由器里。 实际上，当应用程序希望能与其他设备进行通信，调用运输层，给运输层一堆信息。运输层可能会把这些信息分段，然后加上头部信息成为报文段。报文段交给网络层，网络层再封装成数据报再向目的地发送。到了目的地之后做一个相反的动作，送给应用层使用。需要区分清楚的是，制定目的地、决定报文段怎么产生、决定报文段如何合并都是运输层的工作，网络层相当于送信的邮差，只负责传输。但是话又说回来了，网络层更加底层，网络层提供了怎样的服务、有多大的带宽都限制了运输层的服务。 现在因特网的运输层主要有两个协议：UDP、TCP。UDP是不可靠的、多目的地的、无序的传输；TCP是面向连接的可靠的传输。具体特性将在后面讲到。 因特网的网络层协议是IP协议（网际协议）。IP非常不可靠，他是尽力而为交付服务，一不确保报文段的交付，二不保证报文段交付的按序，三不保证报文段交付的完整性，被称为不可靠服务。 TCP与UDP的基本职责是将两个端系统之间的IP交付服务扩展为运行在两个端系统之上的两个进程之间的信息交付服务。将主机之间的交付扩展为进程之间的交付就是运输层的多路复用与多路分解。 运输层的多路复用与多路分解多路复用与多路分解服务是所有计算机网络都需要的。 我们知道，实现运输层的服务需要借助于socket，一个进程可以对应一个或几个socket（？）。在send端，运输层在报文段中加了首部信息之后，把各进程产生的数据无差别的交给网络层；在rev端，运输层从网络层那里拿出数据，解析报文段的首部信息从而==定位socket==，把分解后的报文段信息给对应的socket。 为了实现这种服务，在首部信息里面一定封装有端口号。端口号是一个16bit的数，在0-65535之间，其中0-1023的端口号是周知端口号，他们被保留给周知应用层协议来使用。一般来说，服务器端都是一个固定的端口号，而客户端则是随机分配。关于端口号的更多信息，可以访问RFC 3232获得。 1. 无连接的首先了解UDP的套接字标志方法：UDP的socket以一个二元组(dstaddr,dstport)来标识，与源信息无关。也就是说，虽与一个机器无论是从什么地方发送过来的数据，只要给的是同一个port，都将被同一个socket处理。这样，容易理解，在UDP中，每一个port会对应一个自己的缓冲区。 关于这个缓冲区，UDP不可靠也会体现在这里，在发送UDP包的时候不能过大，也是出于对缓冲区大小的考虑。如果新到来的包太大了、不能塞进缓冲区了，这个包会被直接抛弃。app层不会知道，只有通过发送端的超时重传机制对包进行重传。 2. 面向连接的TCP的套接字标志方法与UDP不同，TCO使用(srcaddr,srcport,dstaddr,dstport)，这样，只有四个值都相同才会被定向到同一个socket，放到同一个缓冲区。 这里只是简单一提：关于安全性的问题。攻击者可以利用在某个端口监听的有缺陷的应用程序攻陷目的主机。我不了解安全，这里只是复述了《自顶向下方法》中的解释。 使用nmap既可以扫描到因特网中任一台主机，顺序的扫描主机的各个端口，对TCP/UDP，寻找能接收TCP连接/能对UDP报文段进行处理的端口号，并返回打开的、关闭的、不可达的端口号列表。 3. Web Server与TCP其实没什么新东西，列举： 连接套接字与进程之间并非一一对应的关系。先进的高性能Web server只有一个进程，但是为每一个连接的新客户（不是客户机喔）创建一个新的线程。 非持续HTTP会严重影响web server的性能。（在应用层中涉及），有些OS技巧可以减轻这个问题的影响。（参见[Nielsen 1997,Nahum 2002]） 他的复用与分解是这样的： UDP概述UDP很弱，除了上一节提到的复用分解机制以及少量的差错检验之外，基本没有功能了，如果程序员选择了UDP，那基本上就是在与IP打交道了。 以一个DNS查询为例：应用层给出了一个查询报文交给运输层，UDP加了首部段，交给网络层，网络层将他封装进一个IP数据报，发送给一个名字服务器。DNS如果没有收到对方的回应，要么向另一个服务器发送请求，要么告知调用它的应用程序没有响应。 1. UDP特点 无连接： 两个UDP之间没有握手； UDP报文段彼此独立 不可靠： 没有确认接收； 没有重传； 没有检查包丢失与包失序 检验和只覆盖部分信息 没有拥塞控制 UDP很弱，为什么不使用更加可靠的TCP呢？ 关于何时发送什么样的数据的控制更为精细。UDP是只要应用层交付，UDP就立即给网络层，而TCP有一个拥塞控制机制，可能会遏制发送方，是数据传输变得缓慢。 无需建立连接，减少时延。 无连接状态，不需要维护连接状态。连接状态包括接收和发送数据缓存、拥塞控制参数、序号以及确认号的参数等。某些应用程序运行在UDP之上可以支持更多的活跃用户。 分组首部开销小 UDP的这些特点使得： DNS简单查询； 流媒体应用； P2P应用 网络管理应用：通产工作在高压状态之下； 路由转换协议 等将有更佳的适用度。 现今，由于出于安全考虑，某些机构阻塞UDP流量，而且当丢包率低时，TCP将更多的用于流媒体应用。 然而，由于UDP没有拥塞控制机制，当网络繁忙时，路由器会有大量的分组溢出，几乎没有UDP分组能够成功通过路由器到达目的地。而且，具有拥塞控制机制的TCP发送方会减慢自己的速率。UDP没有拥塞控制机制不仅造成了UDP会话之间的高丢包率，也挤垮了TCP会话。很多研究人员已经提出了新的机制，使所有数据源执行自适应的拥塞控制。 希望使用UDP实现可靠传输的话需要在应用层协议上加上可靠性保证。这种方法既可以保证可靠传输，又可以拒绝拥塞控制的影响。 表1：流行的因特网应用及其下协议 应用 应用层协议 下层运输协议 电子邮件 SMTP TCP 远程终端访问 Talnet TCP Web HTTP TCP 文件传输 FTP TCP 远程文件服务器 NFS 通常UDP 流式多媒体 通常专用 UDP或TCP 因特网电话 通常专用 UDP或TCP 网络管理 SNMP 通常UDP 路由选择协议 RIP 通常UDP 名字转换 DNS 通常UDP 2. UDP报文 长度：指明了包括首部信息在内的UDP报文段长度（单位：字节：Byte）； 下面重点说检验和： 在计组里面就学了几个检错方法，对于一个检错机制：数据=冗余位 实际数据。决定检错能力的是冗余位数与算法。对于UDP，他提供的是一个很简单的检错机制，而且只能检错，不能修改错误。 Persudo Headerpersudo header结构是这样的： 在发送端，首先会将报文段中的checksum清零，然后把报文段与persudo header的数据当作16bit的很多数据，对他们进行求和运算，将得出的16bit结果求反给checksum字段。 在接收端，产生新的伪首部，然后只需要将这些数据全部求和，最终如果是16个1，说明可能是没错的，但是也可能发生的错误已经超出了检错能力，如果不是16个1，那说明一定是错了。 checksum有什么用呢？对于明确错误的信息，UDP会要么丢弃；要么把数据给应用层，但是给应用层警告说数据是错的。 其实伪首部破坏了分层结构，在运输层产生了网络层的信息。 UDP校验和覆盖的范围超出了UDP数据报本身，使用伪首部的目的是检验UDP数据报是否真正到达目的地，正确的目的地包括了特定的主机和该主机上特定的端口 checksum机制在ipv4下是可选的，但是在ipv6下是必须的。 在checksum中看到了端对端原则。端对端原则是一个被受赞扬的原则，该原则表述为某种功能必须基于端对端实现：”与在较高级别提供这些功能的代价相比，在较低级别上设置的功能可能是冗余或几乎没有价值的。“在这里，由于不能保证从网络到物理到链路都有有效的检错机制，甚至在某些路由器的内存中也可能引入比特差错，实现端对端的错误检测机制是有必要的。 可靠数据传输原理在这一节里，将所有的底层数据传输视为不可靠的，可能会发生数据的丢失、差错、失序等。 这里讨论的是普遍的计算机网络使用的理论，将以分组代替报文段被使用。同时，只考虑单向数据传输，其实双向数据传输并不会更难。 构造可靠数据传输协议rdt1.0：信道完全可靠假如信道完全可靠，UDP完全可靠。发送方只要接收到rdt_send就做处理并调用udp_send，接收方只要接收到rdt_rcv就处理。有限状态机（FSM）图如下： 发送端： 接收端： rdt2.0：信道具有比特差错自动重传请求（Automatic Repeat reQuest）：在接收方确认接收到正确的报文之后给出肯定确认，接受到不正确的报文之后给出否定确认。 实际上，这种机制需要其他的支持： 差错检测 接收方反馈：如果正确，返回ACK，不正确返回NAK。 重传：发送方在接收到NAK时重传。 FSM： 发送方： 接收方： 但是，如果ACK或者NCK的包错误了怎么办呢？一般有三种解决方法： 发送请求，询问上一个差错包的内容。这样很容易陷入询问的死循环。 增加足够的检错纠错能力。导致冗余数据过多。 直接重传。可能会引入冗余分组，使得接收方不知道这个包是新包还是重传包。 对于第三种方法，可以引入序列号的概念来解决问题。对于我们现在讨论的停等协议，只需要两个序号即1bit即可。 这个版本是rdt2.1，新的FSM是这样的： 发送方： 接收方： 其实，如果不使用NAK，而是对上次的接收再发送一个ACK，也能起到同样的效果，这样，有了rdt2.2版本： 发送方： 接收方： 比特交替协议——rdt3.0：信道可能丢包、产生比特差错如果产生了丢包，接收端是不会知道的，只能在发送端处理这个问题。一个是发送端主动发送的数据丢失，和接收端没有关系，一个是接收端已经收到了包，但是发送的确认信息丢失。 发送方需要重传，应该做到： 每次发送数据之后启动一个定时器 响应定时器中断 停止定时器 在重传中，延时时间的设置是很重要的。过长的延迟时间将影响应用层的体验，果断的延迟时间将会引入冗余数据分组。rdt2.2已经有能力处理冗余数据分组：当在等待1的时候又传来了0，接收端会直接丢弃这些数据。而对于停等协议，不会有在等1的时候正好发送了1的冗余包的情况。 发送方： 流水线可靠数据传输协议rdt3.0对于错误处理已经比较完善了，但是他的问题在于他是一个停等协议。停等协议对链路的利用率低到令人发指！ 对于一个速度是1Gbps的链路，发送一个大小1000Byte的包： 进入所需时间：$$t=\frac{L}{R}=\frac{8000bit/pkt}{10^9bit/s}=8us/pkt$$ 假设传输所需时间是15ms，那么信道的利用率是： $$U_sender=\frac{\frac{L}{R}}{RTT+\frac{L}{R}}=\frac{0.008}{30.008}=0.00027$$ 为了提高传输效率，停等协议需要被摒弃。现在使用的技术被称为流水线。使用流水线技术，1个bit的序号是不够用的，在后面我们将讲到对于n个同时发出的包，需要2*n个序列号。而且，在发送端和接收端都需要有缓存机制来暂时存储：发送端需要存储已经发送但是没有确认接收的数据，接收端需要存储已经正确接收的数据。 这一节并没有完整的讨论流水线机制下的协议，从下一小节开始，对缓存的处理、序列号的处理进行讨论。 回退N步：GBN允许发送方连续发送N个数据包，接收方按序接收数据包，失序的包会被丢弃。（这时由于没有考虑双向传输，而且接收端的机制，在接收端的结构暂且不提） 在发送方将会有一个缓冲区，把结构抽象成这样： 在这次传输的过程中只能使用[send_base,sendbase+N-1]之间的序号。扩展FSM如图： 发送端： 接收端： 在这个FSM中存在变量，初始化是需要的。在发送端，当上层请求发送数据时，有一个判断：如果还可以继续发送（nextseqnum&lt; base+N），就继续发送，如果在发送这个之前所有的包都已经被确认过了，就需要开一个定时器，从这里可以看出：计时器以最左边界启动。如果不能再发送了，拒绝发送数据。这时可能有三种处理方式：将该数据返回给上层，隐含地表示已经不能再传了；缓存这些数据；使用同步机制只允许上层在窗口不满的时候才能调用rdt_send。数据发送出去之后，如果按时接收到了一个ACK，比如说是ACK(i)，那么说明从base到i都已经被确认了。假如这时已经没有未确认包了，计时器停止，否则，重新为新的最左包启动一个定时器。如果ACK损坏了，什么也不做。因为有定时装置，只需等待重传。这与rdt3.0的机制是相同的。好了，timeout了，将现在所有的未确认全部重传，然后重启定时器。 在接收端，同样初始化。接收端只会等待需要等待的按序的包，如果不是按序的，不予接收，并将上一个确认信息给发送方。类似于rdt2.0的处理方式。发送方收到的这个包并不会对发送方有什么影响，只是由于timer不能被停止或重启，一定时间之后便会触发重传。但是对于那些ACK丢失的包有重大意义：推动sendbase向前。如果接受端收到了一个按序的、在检错能力之内没有错误的包，才对他进行处理，更新自己的期望包序号，并向发送方发出自己的确认信息。 这种方式的弊端在于失序数据的丢弃。 假如发送方一口气发送了N个包，恰巧第一个包坏了，其他都好的，由于接收端拒绝接受第一个坏的包，其他包也被丢弃了，只能重传所有的N个包。或者，其实这N个包都好好的到了接收端，接收端返回了N个确认包。 emmm其实这里做的比较好，因为反回了N就代表N以及以前的都已经收到了。 Go-Back-N动画 选择重传对上一小节最后提出的回退N步的弊端，在选择重传之中得到了解决。但是应该知道，功能的增强需要结构的复杂作为代价。 允许失序，接收方会对一些失序的包进行缓存。这样，确认接收的机制也需要进行修改、接收方也需要拥有一个缓存区来实现。 在这种方式中，其实在上一节最后说的那点优点是不可能存在了，因为需要选择重传，必然是选择确认。而且，每个分组都必须有自己的定时器，而不能像上面一样按最左来做定时器。 在发送端，base只能移动到没有已发送但未确认的最右。对于上图，发送端的绿色必定对应接收端的粉色或者无色，接收端的灰色必定对应发送端的黄色，在sendbase右边不可能有对应接收端灰色的色块。 发送方的动作与前面类似，只是需要将确认机制修改成单独确认，接收方的动作如下： 如果收到了[recvbase,recvbase+N-1]内的包，是正常的，把收到的包的对应序号ACK，如果在这之内的某个包出了问题不用管，丢掉。如果等于recvbase了，就更新recvbase。（这也使得2.是必要的） 收到了[recvbase-N,recvbase-1]之内的包，可能是这之内的包已经被接收端ACK了，但是没有被发送端ACK，一定需要传一个ACK包给发送端，否则会使得sendbase无法前进。 其他情况忽略即可。 情况2.也说明了为什么需要2*N个序号： 尴尬的情况：究竟是重复包还是新包？ 在书中提到了分组重排，我不是十分理解： 分组重拍的一个表现是具有确认号x的旧副本可能会在网络中出现，即使现在发送端与接收端的窗口中可能都包含x。对于分组重排，信道可被基本看成是在缓存分组，并在将来的任意时刻自然地释放这些分组。因为序列号要被重用，所以这种情况要十分小心。实际采用的一个方法是直到发送方确认网络中不会在存在x分组。这通过设置一个分组在网络中的最大存活时间来限定。在高速网络的TCP扩展中，一般是3min。[sunshine 1978]描述了一种使用序号的方法，可以使重新排序问题完全被避免。 Selective Repeat动画 这些都搞清楚了之后，TCP隆重登场~ TCP：Transport Control ProtocolTCP是面向连接的协议，这是因为TCP在传输数据之前需要进行三次“握手”，它传输的是字节流，也就是收TCP并不管传输内容的有意义的分割，只是够了可以出发的字节数（或者不够，这是其他的机制）就会将这个报文段发出，这就要求了接受的数据必须能够按序排列好。 TCP的传输有以下特点： 流量控制 拥塞控制 点对点 全双工服务 流水线机制 TCP称得上是最复杂的协议之一，由于TCP的底层是IP协议，而上面已经提到IP协议是“尽力而为”服务，几乎没有可靠性的保证，我们只能假设底层是完全不可靠的服务，由此构建出了TCP协议。 基本概念介绍TCP连接并不是一条实际的连接或是一条虚电路，所谓的TCP连接只是存储在端到端系统中的连接状态，而路由器、链路层交换机对TCP连接完全视而不见，他们看到的只是数据报。 全双工服务（full-dumplex service）：指的是只能AB之间互相传输，是点对点，不可能有第三者加入的情况出现。与多播有明显不同。 客户进程：发起连接的进程；服务器进程：接收连接的进程 三次握手：客户首先发一条特殊报文，服务器用另一个特殊报文来响应，最后客户用第三个特殊报文响应。$\color{red}{前两个报文不承载有效载荷，而最后一个可以有}$ 当app层给运输层TCP一些数据，会被放入发送缓存里，在TCP方便的时候，就会从缓存中拿出一些数据发送。 最大报文段长度MSS（Max Segment Size）常根据最大传输单元设置，典型值是1460字节。注意不包括各种首部，只包括应用层数据大小。TCP/IP首部一般是40bytes。 最大传输单元MTU（Max Transmission Unit）：本地主机发送的最大链路层帧长度 TCP报文段：TCP将这次要发送的数据用一个TCP首部进行封装。 就像前面说到的，MSS限制了TCP报文段的最大长度。当一个大文件被发送的时候，它将被分为许多长度为MSS的报文段发出。另一方面，对于那些不需要传递大量数据的交互应用，也可以发送较小的数据段。那么什么叫TCP方便的时候呢？一般来说，TCP会在三种情况下发送数据： segment full 超时 app层的push 发送与接收动画 个人觉得（未经过资料查证，只是陈述想法），TCP连接就只是一种连接状态：recv与send都知道对方时刻准备着接受自己的数据，自己发出的数据可以说送到正确的地方，这就够了。TCP连接组成包括：这对主机各自的：缓存、变量、进程连接的套接字。 TCP报文段与UDP的报文段对比起来，TCP报文段可以说是非常复杂了。 源端口号、目的端口号：TCP使用四元组确定一个连接，该信息用于复用分解。序号、确认号：像前面讨论到的保证可靠性的机制一样，TCP使用序号来确定每一个字节，注意TCP为每一个字节分配一个序列号，而不是像rdt一样为每一个包分配一个序号。这个机制将在后面提到。确认号：TCP确认已按序收到的下一个字节。接收窗口：用于流量控制，接收方告知发送方醉倒还能给我发送多少字节。首部长度：一般是20字节，比UDP多12字节（Byte）。选项可令TCP首部长度发生改变，首部长度按照32bit的字为单位。因特网检验和：同UDP。checksum紧急数据指针：指向app层标识的紧急数据的尾字节。当紧急数据存在并存在紧急数据指针的时候，TCP必须通知上层。该字段一般不用。选项：用于双方协商MSS的大小、时间戳等。可参考RFC 854与RFC 1323了解细节。URG：urgent，紧急。指示存在紧急数据。ACK：指示该报文段包括一个对已经被成功接收的报文段的确认。PSH：要求将数据立即给APP层。RST：在连接出现问题时重置。SYN：连接建立请求。FIN：终止连接请求。 需要注意的是：TCP的确认是累积确认，也就是只会确认到有序的字节流的下一个。对于那些失序的包，选择权交给了实现TCP的编程人员： 立即丢弃 缓存：这是实际中做的。 $\color{green}{关于究竟缓存是怎么实现的，需要再查阅资料。}$ 稍带确认：一方向另一方发送信息时顺带确认另一方向自己发送的信息。每一次发送的TCP报文段都必须有SEQ与ACK字段。当没有新数据到达时，SEQ就为下一个期待的字节的序号。 往返时间估计与超时往返时间（RTT）的估计是为了能够设置一个良好的超时重传的时间，以避免时间太短带来的不必要的重传以及时间太长而影响应用层的体验。 SampleRTT指的是某TCP报文段从被发出到收到确认之间的时间间隔。 TCP在任意一个时刻，只为当前已经发送但仍未确认的一个TCP报文段计算SampleRTT，得到一个新的SampleRTT值。TCP不会为重传的报文段计算RTT，它只会对第一次上传输的报文段计算，这是为什么？ 由于路由器的阻塞以及系统负载的变化，SampleRTT将会随之波动，因此采取了一种对SRTT取平均的方法，委会一个EstimatedRTT。$$ERTT=(1-\alpha)*ERTT+\alpha*SRTT$$ $\alpha$通常取0.125，故：$$ERTT=0.875*ERTT+0.125*SRTT$$ 这种指数加权平均移动将使得之前的SRTT的权值快速减小。 DevRTT是RTT的偏差，用于估算SRTT偏离ERTT的程度： $$DRTT=(1-\beta)*DRTT+\beta* | SRTT-ERTT|$$ $\beta$通常取值0.25，故： $$DRTT=0.75*DRTT+0.25*|SRTT-ERTT|$$ $$\color{red}{千万不要忘记给公式里面的markdown符号加转义谢谢}$$ 最终的超时时延应该设置为：$$TimeoutInterval=ERTT+4*DRTT$$ 推荐的初始TI是1s。 快速重传：当发送方连续三次收到对同一个字节的确认消息，默认为是对下一个字节的NAK，将会忽略定时器，立即对下一个报文段进行重传。对于有问题的报文段，TCP并不管是丢失、数据错误、ACK损坏，都以重传报文段来解决问题。TCP使用流水线，可以显著的增加吞吐量。一个发送方可以具有的未被确认的报文段数量由流量控制与拥塞控制机制决定。 可靠数据传输TCP的可靠数据传输服务保证另一个进程从其接收缓冲中读出的数据无损坏无间隔非冗余的按序字节流。 提出问题：对于那些被接收方缓存但是由于无序不能确认的包，有什么机制呢？（这个不同于回退N步，亦不同于选择重传（选择重传N个定时器）） 简化的TCP发送方动作12345678910111213141516171819202122232425262728293031sendbase = initial_sequence number nextseqnum = initial_sequence number loop (forever) &#123; switch(event) /* event: data received from application above create TCP segment with sequence number nextseqnum compute timeout interval for segment nextseqnum start timer for segment nextseqnum TCP只会为第一个设置一个定时器 pass segment to IP */ ; nextseqnum = nextseqnum + length(data) ; /*event: timer timeout for segment with sequence number y retransmit segment with sequence number y compute new timeout interval for segment y restart timer for sequence number y */ //event: ACK received, with ACK field value of y ; if (y &gt; sendbase) &#123; /* cumulative ACK of all data up to y */ cancel all timers for segments with sequence numbers &lt; y ; sendbase = y ; &#125; else &#123; /* a duplicate ACK for already ACKed segment 快速重传机制*/ increment number of duplicate ACKs received for y ; if (number of duplicate ACKS received for y == 3) &#123; /* TCP fast retransmit */ resend segment with sequence number y ; restart timer for segment y ; &#125; &#125; /* end of loop forever */ &#125; 超时间隔加倍前面讲到TimeoutInterval根据ERTT与DRTT推导出，其实这只是在首次发出报文段的时间间隔。之后每超时一次，这个时间间隔都会被设置为原来的2倍。这也算是拥塞控制的一部分，因为当数据报丢失时，很可能是因为网络的拥堵。在后面将会更加详细的介绍TCP拥塞控制机制。 快速重传就像前面已经提到的，3个对同一数据的冗余ACK激发快速重传。 GoBackN or Selective Repeat？TCP看起来更像是Go Back N，因为只有一个定时器，但是TCP又会在接收方提供缓存机制，而且每次重传都只会重传一个报文段。 对TCP提出的一种修改意见是选择确认，也就是允许TCP对失序报文段有选择的确认。这种机制我们不讨论。 云巽：yunxun，意外发现了一个很好听的词！ 流量控制emmm这里是想直接嵌入js的，没想到失败了。就先这么放着，在文章最后可以看到。 需要注意的一点是，流量控制与拥塞控制并不是同一个概念。流量控制是为了使接收方的缓存区不会溢出，拥塞控制则是为了降低网络的负担。 TCP的发送端维护接收窗口rwnd，接收窗口将指示给发送方接收方还能最多收到多少字节的数据。定义：LastByteRead：接收方的应用进程从缓存中读出的最后一个字节的编号。LastByteRcvd：从网络到达接收方并放入接收缓存的最后一个字节的编号。接收缓存RecvBuffer。由于TCP不允许溢出：LastByteRcvd-LastByteRead&lt;=RecvBuffer接收窗口的大小：rwnd=RecvBuffer-(LastByteRcvd-LastByteRead) LastByteSent：发送方维护的已经发送的最后一个字节的编号。LastByteACKed：被确认的最后一个字节的编号。 根据接收方的动作，由于被Read的一定是已经接收方被ACKed的（但是发送方可能会因为其他问题还没有被ACK），因此，只需要保证LastByteSent-LastByteACKed&lt;=rwnd，这样就可以保证接收方的缓冲区不会溢出。 动作：接收方每次ACK，都告诉发送方自己的rwnd，发送方将最多发送rwnd字节的数据。 仔细想一想，其实这个真的是有用的吗？如果一个包ACK了N字节，但是ACK损坏掉了，这时可以看到rwnd指示有额外的空间，但是在发送方却没有额外的空间了（sent-acked=rwnd），这时应不应该继续发送呢？ 这种方法其实有一个问题：接收方ACK的时候，告诉发送方自己的rwnd=0，过了一会儿，APP层拿走了一些数据，这时rwnd有空间了，但是发送方却不知道有空间了，不会发送数据。这就会形成死锁。有两种可能的解决方法，但是TCP要求接收方尽可能简单，于是这个任务落在了发送方。发送放在被告知对方的rwnd=0之后一段过时间将开始向接收方发送只含有一个字节的探测报文，这些报文段将会被接收方确认，直到发送方接收到了一个rwnd!=0的报文段ACK。 UDP不提供流量控制。 TCP连接管理连接建立 三次握手如上： 客户端发送一个不承载数据的SYN被置为1的报文段SYN报文段。client_isn是随机生成，为了消除分组重传带来的错误（取决于网络时延）。有一些在如何选取client_isn方面的有趣研究。 服务器端发送SYNACK报文段：一旦服务器端接收到了上一个报文，就开始准备为这条连接分配缓存与变量，并发送一个允许连接的报文段：SYN被置为1表示请求连接。（TCP连接互不干扰，实际上存在两条连接，在关闭连接时更能体现出），seq指明了自己的初始序号，ACK是对前面一个请求连接报文段的确认。此后，客户端就可以向服务器端发送信息了。 客户机为这条连接分配变量与缓存，并发送确认报文：是客户端确认服务器端连接的普通报文段，这个报文段可以承载数据（因为client-&gt;server的连接已经建立）。此后，服务器端就可以向客户端发送信息了。 client_isn以及server_isn一般有自己对应的counter产生，从而确保相近时间创立的连接的seq不同。 可以想象，如果在A到B的使用到了seq=N的连接刚刚关闭，新开的A到B的相同端口号的连接又使用了序号N，而由于网络时延的关系，上一个连接的N还存在于网络之中，就有可能出现上一个N被误当作这次的N被接收ACK，导致应用层获取数据错误的情况发生。 上面的是一个标准的打开过程，实际上，打开过程可能会出现各种幺蛾子，比如： Simultaneous Open ：实际上，这种打开是成功的。两端都有各自的连接，实际上所起的效果是一样的。这里的第二个报文之所以没有seq，是因为seq是给对方ACK使用的，由于这里的每一条连接是单向的，就不需要给出一个新的seq。 半打开：半打开是失败的打开，是因为一方的连接建立起来之后另一方没有再回消息，也就是第三条报文不见了。比如机器非正常关机等都可能会导致半打开。数量较大的版打开将会影响效率与空间。怎么消除半打开的连接呢？可以定时检查，向对方发送报文，如果不如回应，就把己方的连接关闭。 连接关闭参与TCP连接的任意一方都能随时终止连接，连接结束之后，为连接维护而存储的变量与缓存都将被释放。红色部分之后，client不再发送报文段，但是还可以接收；绿色之后，server不再发送报文段，连接正式关闭。 半关闭：半关闭是正常现象，指的就是A-&gt;B的连接关闭之后，A不会再发送但是还能接收，B到A的也关闭了之后才都消停下来。 TCP连接状态序列客户机的TCP状态序列： 为什么直到等待一个IP数据报在Internet上可能存活的最大时间长的两倍（即120秒）后，一个连接才能从TIME_WAIT状态转移到CLOSED状态？当连接的本地一方已经发出一个ACK数据段响应对方的FIN数据段时，它并不知道这个ACK数据段是否成功地被传递。结果是，另一方可能有重传一个FIN数据段，而这个第2个FIN数据段可能在网中被延迟。如果允许连接直接转移到CLOSED状态那么可以会有另一对应用进程会打开同一个连接（即使用同一对端口号），而前面连接实例中被延迟的FIN数据段这时会立即使后来的连接实例终止。同时也避免了分组重传带来的错误信息。 服务器的TCP状态序列： TCP定时器总结Connection Establishment Timer：当SYN包发出时，连接建立定时器就开始计时，如果在75秒内未收到响应，则连接建立失败。The Retransmission Timer：重发定时器是TCP发送数据时设置的，如果数据在重发定时器超时时还没有返回确认，TCP就重发数据。定时器的设置是动态的，它基于TCP对往返时间（round-trip time）的测试，重发时间设在1到64秒Delayed ACK Timer：当TCP实体收到数据时它必须返回确认，但并不需要立即回复，它可以在500ms内发送ACK报文，如果在这段时间内它恰好有数据要发送，它就可以在数据内包含确认信息，因此需要ACK延时定时器。The Persistence Timer：管理一种较为少见的事件，即死锁情况。为了让发送方暂停发送数据，接收方发送一个接收窗口为0的确认。后来，接收方又发送了一个更新了窗口大小的分组，但该分组丢失，于是，双方都处于等待为了防止上述事情发生，发送方在收到接收方发来一个窗口为0的数据时，就启动持续定时器，等该定时器超时还没有收到对方修改窗口大小的数据的话，发送方就发一个探测数据，对该探测数据的响应应包含了窗口大小，若仍为0，则定时器清0，重复以上步骤，否则则可以发送数据。The Keep-Alive Timer：当一个连接长时间闲置时，保持存活定时器会超时而使一方去检测另一方是否仍然存在，如果它未得到响应，便终止该连接The Quiet Timer：当TCP连接断开后，为防止该连接上的数据还在网络上，并被后续打开的相同的连接接收，要设置闲置定时器以防止刚刚断开连接的端口号被立即重新使用 SYN洪泛攻击在上面提到了：半Open。其实这种情况大都在一分钟之后被服务器端发现并断开连接，回收资源了。但是这一分多钟的时间的资源浪费将为经典的Dos攻击即SYN洪泛攻击创造了环境。在这种攻击模式中，攻击者发送大量的SYN报文段而不发送最终的ACK，导致大量的半连接占用服务器的资源。现在有一种有效的解决方式：SYN cookieSYN cookie的工作方式是：当server接收到一个SYN，他并不立即为这个连接分配缓存与变量，而是使用这个TCP报文段携带的源ip与源port信息，运行一个仅有server知道的一个散列函数得到一个初始序列号，这种精心制作的初始序列号被称为cookie。server仅仅是发送一个具有该序号的SYNACK。该序号也不会被保存。如果一段时间之后接收到了同一个源ip与源port的ACK报文段，server发现二者并没有连接，于是再次运行散列函数，得到的序号如果加一就是本ACK携带的序号，server就认为这是一个正常的client，为之分配资源，建立连接，否则，server不会分配资源，之前的攻击没有任何影响。（哎呀其实也稍稍影响了处理效率了吧） 其他情况前面考虑的都是特别理想的情况，如果client向server一个并不接受TCP连接的port发送了一条报文将会怎样？ server将发送一个特殊重置报文段给client，这个报文段将RST标志位置为1，告诉client自己没有那个port接收TCP，请他不要再发送了。 如果发送的是一个UDP报文段，如果不匹配，server发送一个ICMP数据报。这在网络层进行讨论。 nmap工作原理：nmap向某一台主机的一个port发送一个SYN，接下来有三种情况：收到一个SYNACK，则会标明那个端口上有一个已经打开的TCP应用程序，返回“打开”，如果收到RST，说明这个端口没有一个TCP应用程序，如果什么都没有收到，说很可能是被防火墙给墙了。nmap下载 TCP option 先只放一个图。 拥塞控制原理在讨论TCP的拥塞控制之前，首先来讨论更加普遍的拥塞控制原理。 我们把分组重传作为网络拥塞的信号。实际上，分组重传不全是因为网络拥塞。 ATM：异步传输网络ABR：可用比特率 1. 两个发送方与一个无限缓存的路由器不考虑分组重传、流量控制、拥塞控制，忽略首部信息的额外开销。 这种情况下，发送方完全不节制，每连接的吞吐量如下：吞吐量不可能有超过R/2的稳定状态，这是因为二者共享一个吞吐量R的链路。当发送速率在[0,R/2]时，接收方的吞吐率等于发送方的发送速率，也就是发送方的所有数据经过有限时间传输之后到达接收方。 但是，再看速度与平均时延的关系：看起来吞吐量接近R/2是件好事，但是从时延的角度来看，接近R/2的时候，平均时延就会越来越大。 代价1：巨大的时延。 （为什么？？？到R/2的时候不是正好利用完所有链路吗？又没有重传，时延为什么会这么大？？？ 2. 两个发送方以及一个具有有限缓存的路由器假设：路由器缓存已经满了的话，新来的数据包就会被丢弃。 供给载荷：运输层向网络中发送报文段的速率。 仍然使用上面的图。 如果控制得非常好，yin=yout 如果只对丢失的包进行重传，yin`&gt;yout 对迟到的包重传，yin`远大于yout 以上三种情况下的吞吐量随着供给载荷变化：（所谓吞吐量，就是☞yout吧） 第二图的R/3-R/2是部分重传丢失占用的带宽，第三图的R/4-R/2是部分重传丢失+延时重传带来的丢失。 代价2：不必要的重传占用带宽 3. 四个发送方以及有有限缓存的多台路由器及多跳路径 从虚线开始，吞吐量就开始下降，一直到最后几乎为0.这时路由器几乎一直在丢失重传丢失重传。 拥塞控制方法 端到端的拥塞控制：IP协议并不提供对拥塞控制的支持，如果要在运输层做拥塞控制，也只能通过行为来判断。例如3次同一数据的ACK，被认为是丢失报文段，进而以为网络有些繁忙。 网络辅助的拥塞控制：路由器向发送方提供显式的拥塞信息。这个信息只需要一个比特。有两种方式：一种是直接反馈，路由器直接告诉发送方拥塞；另一种是间接反馈，路由器标记数据，当接收方收到被标记的数据之后在ACK的时候告诉发送方。 网络辅助的拥塞控制：ATM ABRTCP拥塞控制TCP的拥塞控制的基础是认为所有数据包的丢失都是路由器因为阻塞给我丢掉了。实际上，有线网络传丢的可能性很小，所以这个基础是没什么问题的。但是在无线网络中，TCP的拥塞控制则需要有一定的改进。 TCP拥塞控制概述拥塞窗口cwnd：发送方维护一个变量，他对一个TCP发送方能对网络中发送数据的速率进行了限制。特别的：$$LastByteRcved-LastByteACKed&lt;=min{rwnd,cwnd}$$ cwnd限制了发送方每次向接收方发送的字节数量，考虑一个丢包与发送时延皆忽略不计的连接，粗略的讲，发送方发送的速率大概是（cwnd/RTT)字节/秒。因此，调整cwnd可以调整发送速率。 那么发送方要怎么知道有了拥塞？第一就是像前面所说的丢包或是连续三个ACK。有没有可能丢包不是因为网络拥塞呢？当然是可能的！ 对于不丢包的理想情况下，TCP的发送方将会收到对以前未确认报文的确认，它们使用这些确认来增加cwnd。到达的速率将会影响cwnd增长的速率，因为TCP使用触发（或计时）来增大它的拥塞窗口长度，TCP是自计时的。 TCP有一些处理方法： 一个丢失的报文段意味着阻塞 一个确认报文段指示网络正在向接收方交付发送方的报文段，因此当对先前未确认的报文段到达时，增加发送速率 宽带探测。 TCP拥塞控制算法慢启动： 一条TCP连接开始时，首先cwnd会被设置为1个MSS。1个MSS真的很小，因此TCP此后每当一个报文段首次被确认（注意不是收到ACK哦，是被确认哦）就把MSS增加1.这样1变2，2变4，4变8…指数增长。 什么时候停止这种爆炸增长？ 存在丢包事件，将ssthresh置为cwnd/2，将cwnd重新置1，重新慢启动，。 存在丢包事件，将cwnd的值置为ssthresh，结束慢启动，进入拥塞避免阶段。 检测到3个冗余ACK，执行快速重传，进入快速恢复。 拥塞避免： 进入拥塞避免之后，离拥塞可能并不遥远。每个RTT将增加一个MSS。例如一个RTT发了10个包，那么这十个包只将cwnd增加1个MSS。 什么时候结束这种线性增长？ 丢包，将cwnd设置为1个MSS，sstresh设置为cwnd/2。 三个冗余ACK，将cwnd减半（三个已经收到的ACK要加上3个MSS），将ssthresh更新为cwnd/2，进入快速恢复 快速恢复： 对于引起TCP进入快速恢复的报文段，对每收到的一个冗余ACK都加一个MSS。这个阶段时说明网络其实并不是特别堵，只是传丢了一个，不用太过限制。直到最终丢失的报文段ACK到达，TCP在降低cwnd之后进入拥塞避免阶段。如果出现超时事件，将将cwnd设置为1个MSS，sstresh设置为cwnd/2，进入慢启动。如果出现冗余丢包事件，将cwnd设置为1个MSS，sstresh设置为cwnd/2。 TCP Tahoe与TCP RenoTahoe不管发生什么事件，都会无条件的将cwnd变为1，然后重新慢启动。Reno综合了快速恢复。 Tahoe的过程：在4时达到初始阈值，在8时有三个冗余ACK，在10.5时达到阈值。Tahoe不能容忍丢包，一旦丢包，就会进入慢启动。 123456789/* slowstart is over */ /* cwnd &gt; threshold */Until (loss event) &#123; every w segments ACKed: cwnd ++; &#125;//拥塞控制阶段;threshold = cwnd /2;cwnd = 1;perform slowstart//重新慢启动; Reno过程：在4时达到初始阈值，在8时有三个冗余ACK，之后减小cwnd，进入快速恢复。Reno分超时丢包与冗余丢包，冗余丢包进入快速恢复，超时慢启动。 123456789101112/* slowstart is over */ /* cwnd &gt; threshold */Until (loss event) &#123; every w segments ACKed: cwnd ++ &#125;//拥塞控制;threshold = cwnd /2If (loss detected by timeout) &#123;//进入慢启动; cwnd = 1 perform slowstart &#125;If (loss detected by triple duplicate ACK)//进入快速恢复 cwnd = cwnd /2 于是发现：什么对超时都是没有容忍度的，直接会慢启动。但是对于3个冗余ACK的表现个不一样，一般进入快速恢复。 公平性：TCP与UDPTCP是自律的，如果UDP不自律，可能会挤跨TCP。因此在使用UDP的APP层应该做类似的”平滑、缓慢变化“的拥塞机制，保证公平。 端口与进程最后，我需要补充一下端口号与进程之间的关系： 首先：缓冲区对应的是谁？在上一篇中就应该明确，是socket。 端口号是什么？ 在网络技术中，端口大致有两种意思：一是物理意义上的端口，比如，ADSL Modem、集线器、交换机、路由器用 于连接其他网络设备的接口，如RJ-45端口、SC端口等等。二是逻辑意义上的端口，一般是指TCP/IP协议中的端口，端口号的范围从0到65535，比如用于浏览网页服务的80端口，用于FTP服务的21端口等等。我们这里将要介绍的就是逻辑意义上的端口。 服务器一般通过知名端口号来识别。例如，对于每个TCP/IP实现来说，FTP服务器的TCP端口号都是21，每个Telnet服务器的TCP端口号都是23，每个TFTP(简单文件传送协议)服务器的UDP端口号都是69。任何TCP/IP实现所提供的服务都用知名的1～1023之间的端口号。这些知名端口号由Internet号分配机构（InternetAssignedNumbersAuthority,IANA）来管理。 端口号的作用正是为了区分不同的网络服务。 上面的比较官方，下面将由我自己写的程序来—— TCP里面，一个端口号可以被多个socket绑定。而UDP不行，我的理解是数据不知道要给谁。 已经被绑定之后，再次要求绑定： 这是再次运行，也就是说，其实这个并不是一个进程不能绑定同一个端口号，而是两个UDP的socket不能绑定同一个端口。跟进程没有什么关系。fork子线程的时候，也可以使用其他的socket绑定其他的端口号。 端口这个概念就是计算机网络领域的概念，有网友直言说进程与端口号没有什么关系，其实是对的。 thanks for ur help! [1] 进程与端口映射 [2] 端口号是什么？ [3]IBM Knowledge Center 这个有很多东西，不止端口号 [4]TCP]]></content>
      <categories>
        <category>计算机网络</category>
        <category>大三的课</category>
      </categories>
      <tags>
        <tag>网络课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[the One使用记录]]></title>
    <url>%2F2017%2F11%2F01%2Fthe%20One%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%951%2F</url>
    <content type="text"><![CDATA[一个小tip：使用linux命令行进入带空格的文件，如果直接cd filename进入的话是不行的，那应该怎么进入呢？ cd “filename”就好啦！ 又一个小tip：不下心关掉了eclipse的项目工程目录怎么办？ Window-&gt;Show View-&gt;Others-&gt;General-&gt;Project Explorer emmm，markdown行内代码形式：``包起来就行 从github下载the-one犹记我初学git的时候也是傻傻的，这里就放出来具体操作方法了！ 首先需要一个GitBash。在gitbash里使用cd命令进入想存放the one的文件夹，然后输入命令：git clone https://github.com/akeranen/the-one.wiki.git，前面两个是命令，最后一个是参数，代表的是这个仓库的地址。这个下载下来的是master….应该是不行的….下载v1.6吧。 等到完成之后就会发现在这个文件夹下面多了好多东西呀！ done！ 将项目import进Eclipse并运行eclipse，使用java的孩子应该没有不知道的吧。哦对，the one是java代码编写。 到了这一步就出了一点小问题…一直是使用eclipse的File-&gt;import功能向workspace导入外部的项目的，导入the one的时候竟然告诉我未找到项目文件！我尝试使用File-&gt;Open Project From FileSystem来，这次可以了。the one 成功出现在了project explorer里面！但是！有错误！ 这个应该在说明文档里面说了，1.1以后版本的java要编译theone需要添加一些liabrary到项目的build path，但是我看了我的build path，已经添加了呀？怎么还有错？ click here 在上述网站，第二个回答解决了这个问题。由于JUnit3.0与4.0都在，我就都删除了，然后重新添加了JUnit4.0.done！ 如图，现在整个项目都已经没有错误了！ 选择整个项目，右击Run As-&gt;JUnit Test成功！ the one涉及到了JUnit，如果不了解JUnit，需要先了解一下~ click here~ JUnit之一 在我以JUnit Test来运行时，theone出现了两个fail。这表明….可能是有bug的啊….mark 好吧，我先忽略了， 然后 run了下（主类是core.DNTSim），发现其实可以出来东西。由于可能需要命令行参数，我又使用cmd来编译运行，结果… 但是eclipse成功了呀？我找到了一个在eclipse设置命令行参数的方法，企图先跳过jdk版本带来的（？）编译问题。 DTNSim的API Run-&gt;Run Configurations-&gt;DTNSim-&gt;Argumets:加上你想要的参数。即可： 好了，至此，程序初步可以运行使用了。 解决问题时间 好的，现在是解决问题时间啊。 询问了老师，老师说这个可能是JUnit包加载不正确。但是我重新写了一个工程，使用一样的方法，那个是没错的。 所以我想是不是版本问题。 于是我下载了v1.6。使用同样的方法import进来，这次错误更多！但是我发现这次最初是没有添加README里面说要自己添加的2个jar包的，所有感觉有希望！ 添加jar包： Project右键-&gt; build path -&gt; configure build path-&gt;libraries-&gt;add jars-&gt;the-one-1.6.0-&gt;lib 好了，把这两个jar都添进来。 JUnit的添加方法还与上面一样，记得要添加的是4.0 没错了。 然后来测试一下。 还有错:-)。 既然是有错的，那就只能一点一点分析是怎么回事了。 首先，我又运行了1.6的compile，还是不行，这次我决定把电脑里面的jdk换成8。 如图，配置好的环境变量在cmd竟然没有更换！这也是导致javac不能用的一个原因。如果你确认自己配的环境变量肯定是对的，关掉cmd重新打开，就可以了。 环境变量配置应如下：这次compile的结果。（jdk9不支持那个选项了？好了，这次使用one.bat也可以运行了。（jdk9：exm？？？ but….eclipse测试还是没过…. C:\ProgramData\Oracle\Java\javapath 了解运行方法与配置信息将参数设置为：[-b times] [ur conf_files] -b后必须跟一个数字，这个数字表示simulation将会在batch mode（批处理）下运行，不会出现可视化界面，而是打印信息。后面的数字代表运行次数或者是num1:num2的形式来确定一个范围。 conf_files是一系列你自己写的conf file的文件名，数量任意。如果没有这个参数，会使用default_setting.txt。需要注意的是，如果多个conf设计同一个属性，那么在后面的设定将会覆盖前面的设定。 Conf文件 以下译自README 所有的模拟参数都是通过cond文件给出的。conf文件就是普通的txt文件，在文件里有一些ket-value对。大多数变量的语法都是：Namespace.key = value例如：key的前缀是namespace命名空间，然后加一个.点，然后是key的名字。key和value被等于号=隔开。命名空间需要以大写字母开头，而且namespace和key都以CamelCase方式命名，大小写敏感。namespace宽松地定义了设置影响的某部分模拟环境simulation environment。许多（并非所有）namespace的名字与正在读的class的名字一样。特别是动作模块(movement models)，报告模块(report modules)以及路由模块(routing modules)遵照了这个习惯。有时namespace被用户定义，eg：使用网络接口，用户可以选择任何标识符，在该名称空间中定义特定于接口的设置，并在配置每个组应使用的接口时给出名称空间的名称。 数字可以使用.作为小数点，可以使用kilo(k)，mega(m)，giga(G)作为后缀。bool接受”true”,”false”,”0”,”1”。 许多设置确定了寻找外部文件的路径。路径可以是绝对路径也可以是相对路径，但是路径的分隔符必须是”/“。Unix与Windows都是。 有些变量包含以逗号隔开的值，对他们来说，语法是：Namespace.key = value1,value2,etc. 对于运行索引（run-indexed）值，语法是：Namespace.key=[runvalue;run2value;run3value;etc]。每一个runvalue可以是一个被逗号分隔的值。 设置文件可以有注释，以#开头。 一些值（scenario and report names at the moment）支持”value filling”。有这个特性做支撑，可以从设置的值动态地构造，例如，scenario name。这在使用run-indexing的时候非常有用。Just put setting key names in the value part prefixed and suffixed by two percent (%) signs. These placeholders are replaces by the current setting value from the configuration file. See the included snw_comparison_settings.txt for an example. 这个机制像是：比如，完善java的环境变量的时候：JAVA_HOME=C:/Program Files/Java/path=%JAVA_HOME%jre/…” default_settings.txt这个文件，如果它存在，在运行的时候一定会被read。此后给的文件可以在此基础上设置更多的东西，也可以覆盖掉默认设置的一些定西。 Run indexingrun index可以让你只使用一个conf文件就能运行很多个不同的配置configuration，方法是，你提供一个setting的数组，为在不同的conf文件之间需要改变的变量。例如：如果想用5个不一样的随机数生成器种子来生成movment models来运行模拟器，那么你可以这样define设置文件：MovementModel.rngSeed = [1;2;3;4;5]现在，你使用参数-b 5 my_config.txt运行。 warp-around：类似于OS里面的。也就是说：used values is the value at index (runIndex%arrayLength)。这样，就可以很轻易的运行很多排列。 模块们Movement models在模拟中，运动模型控制着节点的运动。运动模型提供了coordinates坐标、速度、停顿时间。基本的安装包括：random waypoint,map based movement,shorest path map based movement,map route movement,external movement。除了external的所有这些运动模型，都有可控的速度以及停顿时间分布。可以给出最大值以及最小值，运动模型使得值在给定返回之间uniformly distributed均匀分布。在external里面，速度以及停顿时间都由文件中的给定值解释执行（interpreted）。 如果一个节点使用了随机航点运行模型（random waypoint:RandomWaypoint），这个节点在模拟区中被给予一个随机的坐标。节点直接以恒定的速度到达给定的目的地，然后停顿一段时间，然后获得新的目的地。这个过程在整个模拟的过程中持续进行，节点在这些zig-zag（蜿蜒的）路径上走 基于地图的运动模型将节点的运动限制在预先定义好的路径上。可以定义不一样的路径，可以定义对全体节点都有效的路径。这样的话，比如说，汽车才不会直接开进门里或者是开到人行道上。 基本的基于地图的运动模型（map-based:MapBasedMovement）初始时把节点们分布在两个相邻的地图节点之间，然后节点们开始运动，从一个adjacent map node到另一个。当节点到达下一个地图节点的时候，他随机选择下一个节点。但是只有当这是唯一的选项（即，避免回到它来自的地方）时才选择它来自的地图节点。一旦节点移动了10-100个地图节点，它暂停一段时间，然后再次开始移动。 更加复杂的版本的基于地图的运动(ShortestPathMapBasedMovement)使用了Dijkstra的最短路径算法来寻找他在整个地图范围内的路径。一旦一个节点到达了他的目的地，他等一段时间，选择下一个随机的地图节点，用最短路径走过去。（这个算法只对valid map nodes起作用） 对于基于最短路径的运动模型，地图的数据信息还包括POIs（point of interests）。instead of为下一个目的地选择任意随机地图节点，移动模型可以被配置为以可配置的概率给出属于某个POI组的POI。可以有无限数量的POI组，并且所有组可以包含任何数量的POI。所有节点组对于所有POI组可能具有不同的概率。POI可以用来模拟例如商店，餐馆和旅游景点。 基于路线的运动模型（MapRouteMovement）可以用于对遵循特定路线的节点建模。（例如，公共汽车或电车线路。）只需要定义路线上的停靠点，然后使用该路线的节点就会经由最短路径，从一个站点到另一个站点，并在每一个stop停留设置好的时间。 所有的运动模型都可以决定什么时候节点是活跃的（他运动，可以被连接），什么时候不活跃。所有模型，除了external，可以给出多个时间间隔（time interval），在那个组的节点只会在那些时候比较活跃。 所有基于地图的运动模型都通过（WKT）格式的一个子集所规范的文件中获取输入信息。映射路径数据的解析器支持WKT文件的LINESTRING和MULTILINESTRING指令。对于点数据（例如POI），也支持POINT指令。 （MULTI）LINESTRING中的相邻节点被认为形成一个路径，并且如果某些行包含一些具有完全相同坐标的顶点，路径从这些地方加入（这是如何创建交叉点）。 WKT文件可以使用任何合适的地理信息系统（GIS）程序从现实世界的地图数据进行编辑和生成。包含在模拟器发行版中的地图数据使用免费的基于Java的OpenJUMP GIS程序进行转换和编辑。 不一样的地图可以通过在不同的文件中存储属于不同类型的路径来定义。POIs简单的用WTK POINT定义，而POI组通过将所有的属于同一个组的POIs存储在同一个文件中来定义。所有的POI也必须是地图数据的一部分，所以他们可以使用路径进行访问。用LINESTRING定义路线的停靠点，停靠点按照它们在LINESTRING中出现的顺序进行遍历。一个WKT文件可以包含多个路由，并按照它们在文件中出现的顺序将它们提供给节点。 使用外部移动数据（ExternalMovement）的实验移动模型从文件读取时间戳节点位置，并相应地移动模拟中的节点。有关格式的详细信息，请参阅输入包中的ExternalMovementReader类的javadocs。一个合适的实验转换器脚本（transimsParser。pl）的TRANSIMS数据包含在toolkit文件夹中。 要使用的运动模型是使用“movementModel”设置为每个节点组定义的。设置的值必须是movement包中的有效运动模型类别名称。在MovementModel类中读取所有运动模型通用的设置，并在相应的类中读取运动模型特定的设置。有关详细信息，请参阅javadoc文档和示例配置文件。 路由模块以及消息的创建路由模块定义了在模拟中消息如何被处理。6个基本主动路由模块（First Contact,Spray and Wait,Direct delivery, PRoPHET and MaxProp），一个被动路由用于外部路由的模拟。主动路由模块是用于DTN路由的众所周知的路由算法的实现。也有这些模型的变体和包含在最新版本中的几个不同的模型。有关详细信息，请参阅路由程序包中的类。 被动路由器专门用于与其他（DTN）路由仿真器交互或运行仿真，而不需要任何路由功能。路由器除非由外部事件命令，否则不执行任何操作。这些外部事件由实现EventQueue接口的类提供给模拟器。 有两个基本的类可以用作消息事件的来源：ExternalEventsQueue和MessageEventGenerator。前者可以用一个合适的脚本（例如，toolkit文件夹中的createCreates.pl脚本）或通过将例如dtnsim2的输出转换成合适的形式来从手动创建的文件中读取事件。有关格式的详细信息，请参阅输入包中的StandardEventsReader类。 MessageEventGenerator是一个简单的消息生成器类，它创建具有可配置消息创建间隔，消息大小和源/目标主机范围的均匀分布的消息创建模式。可以使用MessageBurstGenerator和One {From，To} EachMessageGenerator类创建更具体的消息传递场景。有关详细信息，请参阅javadocs。 该toolkit文件夹包含一个用于dtnsim2输出的实验解析器脚本（dtnsim2parser.pl）（曾经是一个更强大的基于Java的解析器，但是由于这个更容易扩展的脚本而被丢弃）。该脚本需要dtnsim2的代码的一些补丁，可以从toolkit / dtnsim2patches文件夹中找到。要使用的路由模块是按设置“路由器”的每个节点组定义的。所有路由器都不能正常交互（例如，PRoPHET路由器只能与其他PRoPHET路由器一起工作），所以通常对所有组使用相同（或兼容）路由器是有意义的。 报告可以使用报告创建模拟运行的摘要数据，连接和消息的详细数据，适合使用例如Graphviz进行后处理的文件（创建图表）以及与其他程序接口。有关详细信息，请参阅报告包类的javadocs。 对于任何模拟运行可以有任意数量的报告，并且使用“Report.nrofReports”设置来定义要加载的报告的数量。报告类别名称使用“Report.reportN”设置定义，其中N是从1开始的整数值。设置的值必须是来自报告包的有效报告类别名称。所有报告的输出目录（可以使用“输出”设置对每个报告类别重写）必须使用Report.reportDir -setting进行定义。如果报告类别未提供“输出”设置，则生成的报告文件名称为“ReportClassName_ScenarioName.txt”。 所有报告都有许多可配置的设置，可以使用ReportClassName.settingKey -syntax来定义。有关详细信息，请参阅Report类的javadocs和特定报告类（查找“设置id”定义）。 主机组一个host group是一组分享同样的运动和路由设置的主机。不同的组的设置值不一样，这样的话，它们可以表示不同类型的节点。可以在“组”（Group）命名空间中定义基本设置，不同的节点组可以覆盖这些设置或在其特定的命名空间（组1，组2等）中定义新的设置。 the settings有很多设置可以被设置。非常多，这里不提了。有关详细信息，请参阅类，尤其是report，routing和movement class的javadoc。另请参阅包含设置文件的示例。也许最重要的设置如下。 脚本设置（Scenario setting）123456789101112131415161718192021Scenario.name //脚本的名字，所有报告默认以此为前缀Scenario.simulateConnections //connection是否应该被模拟。如果只对运动模型感兴趣，这个可以被disable，以获得更快的模拟速度。通常情况下，是enable的。Scenario.updateInterval //每次更新时需要几秒。增加数值使模拟更快，但是可能以精度损失为代价。0.1-2 are goodScenario.endTime //How many simulated seconds to simulate.Scenario.nrofHostGroups //现在在模拟中有几个hosts group 接口设置（用来定义node可能会使用到的接口）12345678type //这个接口使用了什么类（来自接口目录）//其余设置是特定于类的。可以是例如：transmitRange //接口范围（meters）transmitSpeed //接口的传输速度（bytes per second） 主机组设置（在Group或GroupN的命名空间使用）]]></content>
      <categories>
        <category>计算机网络</category>
        <category>实验室</category>
      </categories>
      <tags>
        <tag>实验室</tag>
        <tag>计算机网络</tag>
        <tag>模拟器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实验室心情全记录]]></title>
    <url>%2F2017%2F11%2F01%2FCN1%2F</url>
    <content type="text"><![CDATA[又一个小时过去了呢。 第一个看到的文档是：《移动P2P网络中协同缓存的研究》，这个比较浅显比较傻，不是我需要的，主要讲的是一些显而易见的理想化的缓存策略，并没有涉及到一个复杂网络。主要讲的是缓存的接入控制策略：什么东四能进缓存？缓存替换策略以及缓存的一致性。]]></content>
      <categories>
        <category>计算机网络</category>
        <category>实验室</category>
      </categories>
      <tags>
        <tag>实验室</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2017%2F11%2F01%2FCloud%20RAN%20for%20Mobile%20Networks%E2%80%94%E2%80%94A%20Technology%20Overview%2F</url>
    <content type="text"><![CDATA[前言移动数据的传输容量正在连续增加。根据Cisco的预测，在智能手机与穿戴设备的推动下，从2012到2017年容量将增长13-fold。因此，移动网络运营商需要增加网络的capacity来满足日益增长的用户需求。当LTE（long term evolution）的spectral efficiency标准正逼近Shanon极限，增加网络capacity的最好方法要么是增加基站覆盖区域，创建一个结构复杂的多元小基站网络（HetSNets）要么是使用像多用户多输入多输出（MIMO）或是Massive MIMO这样的多根天线在同一频率时段服务于多个用户的技术。然而，这将导致更大的的小基站区间干扰水平（Growing inter-cell interference levels）以及更多的花费。 移动网络的TCO（total cost of ownership）包括了CAPEX（CAPital Expenditure）和OPEX（OPerating EXpenditure）。capex主要指关于网络构建的花费，网络构建可能会从网络规划一直到网址需求，RF硬件，baseband硬件（基带），软件许可，leased line连接（专用网络连接），安装，civil cost（民用花费），网站支持，像供电或者是冷却（cooling）。OPEX包括需要去操纵网络的花费，比如，站点出租，专用网络，电力，运营与保持，升级。CAPEX与OPEX在更多base station被部署之后都显著增加了。更具体地说（more specifically），CAPEX增加是因为基站是无线网络infrastructure基础设施中最昂贵的组成成分，而OPEX增加是因为基站覆盖区需要相当多的电力去驱动，比如，中国移动估计有72%的电力花费在基站覆盖区域（cell sites）。移动网络运营商需要支付网络建造，运营，保持和升级的费用；同时人均收入（ARPU）随之时间的增长保持不变甚至有所下降，因为典型的用户对数据的需求越来越多却希望为移动流量付更少的费用。像图1中所示，如果补救措施，移动网络运营商将有可能面对入不敷出的局面。 因此，在移动网络领域，优化了花费和能源消耗的新型建构方式变得必需。 C-RAN是一种新型的移动网络结构，有着接受上述所有挑战的能力。这个概念最初在论文9中被提出并且在6中被详细描述。在C-RAN中，baseband processing（基带处理）被中心化，被被放在一个虚拟的BBU pool中被诸多site分享。这意味着它可能能够适应nonuniform traffic非均匀流量并更加有效率地利用一些资源，比如，基站。由于这个特性在C-RAN结构中将用到比传统结构更少的BBU，而且C-RAN还有降低网络运营费用的潜能，这是因为电力与能量的消耗相较于传统的RAN结构已经被减少了。新的BBU可以更轻易地被添加被升级，从而改善网络的稳定性并使得网络更容易被维护。虚拟化的BBU pool可以被多个不同的网络运营商所分享，允许他们通过云服务来租RAN。由于多个网站的BBU在同一个pool中co-located（协作），他们彼此之间可以以更小的延迟时间进行通信，这样，被介绍给LTE-Advanced来增加spectral efficiency和throughput吞吐量的机械装置，比如说增强版的ICIC与CoMP都被facilitated，在两个基站覆盖区内实行的负载平衡方法也被facilitated。而且，网络的表现也被改善，例如通过减少BBU pool内部的移交操作的延时。据China MObile Research Institute的猜想，C-RAN结构被移动网络运营商设为了目标，例如IBM、Alcatel-Lucent、华为、ZYTE、诺基亚西门子网络、Intel和Texas instruments。而且，在2020年的水平上来看，C-RAN被视为是典型的在5G中支持soft and green technologies的实现。然而，C-RAN并不是唯一可以对抗上述运营商们面临的挑战的选手，其他解决方式，包括small cells、being part of HetSNets and Massive MIMO。small cells的部署是户外hot spot闹区和室内coverage scenarios平均方案 。]]></content>
  </entry>
  <entry>
    <title><![CDATA[背包问题初涉]]></title>
    <url>%2F2017%2F10%2F31%2F%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[算法上机考试，怎么说，虽然没做出来但是收获很多？好吧也只能这么安慰自己了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;//硬币。存储硬币的价值及其对应个数class Coins&#123;public: int value; int num;&#125;;//更小值函数。若值为0，代表不可能，返回另一个值，否则返回更小值int mini(int a, int b) &#123; if (a == 0)return b; if (b == 0)return a; if (a &gt; b)return b; else return a;&#125;//int max(int a,int b)&#123;// if(a&gt;b)return a;// else return b;//&#125;//int comp(const void *a,const void *b)&#123;// Coins* A=(Coins*)a,*B=(Coins*)b;// if(A-&gt;value&gt;B-&gt;value)return -1;// else if (A-&gt;value&lt;B-&gt;value)return 1;// else return 0;//&#125;//找零钱的函数，coins存储了各种硬币的面值与个数，n是硬币种数（coins的长度），m是找零的钱数。int GetCoin(Coins*coins, int n, int m) &#123; //新建一个二维数组用于存储在每一个要找的钱数下用i种硬币最少用几个硬币 int**M = new int*[2]; M[0] = new int [m + 1]; M[1] = new int [m + 1]; //初始时，用0个硬币找除0以外的钱数，不可能。 //且用0作为初始值便于后续计算。 for (int i = 0; i &lt;= m; i++) &#123; M[0][i] = 0; M[1][i] = 0; &#125; for (int i = 0; i &lt; n; i++) &#123; //对前i种硬币 for (int j = 0; j &lt;= m; j++) &#123; //要找j的零钱 int dola = M[0][j]; //保存未更新之前的值 for (int k = 1; k &lt;= coins[i].num; k++) &#123; //对这种硬币的每个数量 if ((j - k*(coins[i].value) &lt; 0)) &#123; //若k个硬币总钱数大于j，不可能。break M[0][j] = M[0][j]; M[1][j] = M[0][j]; break; &#125; if ((M[0][j - k*(coins[i].value)] != 0)) //否则。若满足条件，说明在M[0][j-k*(coins[i].value)]处有满足条件的解，与M[0][j]计算更好的值。此时这里一定有非0解 M[0][j] = mini(M[0][j], k + M[0][j - k*(coins[i].value)]); else if ((M[0][j - k*(coins[i].value)] == 0)) &#123;//否则，不一定存在非0解 if (k*(coins[i].value) == j) &#123;//若存在k个硬币正好找j元，更新解 M[0][j] = mini(M[0][j], k); //M[1][j] = M[0][j]; //M[0][j] = dola; //break;不行！ &#125; else M[0][j] = M[0][j];//否则，不更新解 &#125; &#125; //将M[0]转移到M[1]，M[0]恢复原值继续下一轮计算。这样可以防止对硬币i使用的数目多于现有数目的情况 M[1][j] = M[0][j]; M[0][j] = dola;// cout &lt;&lt; M[1][j] &lt;&lt; " ,"; &#125; for (int j = 0; j &lt;= m; j++)M[0][j] = M[1][j];//把新值给M[0]，进行对下一种硬币的分析。 // cout &lt;&lt; endl; &#125; return M[0][m];//返回最优解&#125;//int Getcoin(int*coins,int m,int n,int**M,int**N)&#123;// int*resu = new int[n], *resunum = new int[n];// for(int i=0;i&lt;=m;i++)&#123;N[0][i]=0;// M[0][i]=0;&#125;// for(int i=0;i&lt;2;i++)&#123;N[i][0]=0;// M[i][0]=0;&#125;// for(int i=1;i&lt;=n;i++)&#123;// for(int j=1;j&lt;=m;j++)&#123;// //cout &lt;&lt; "the j" &lt;&lt; j &lt;&lt; endl;// if (coins[i - 1] &gt; j) &#123; M[1][j] = M[0][j]; N[1][j] = N[0][j]; &#125;// else&#123;// M[1][j]=max(M[0][j],coins[i-1]+M[0][j-coins[i-1]]);// if(M[1][j]==M[0][j])N[1][j]=N[0][j];// else if(M[1][j]==coins[i-1]+M[0][j-coins[i-1]]) N[1][j]=1+N[0][j-coins[i-1]];// else N[1][j] = N[0][j] &lt; N[0][j - coins[i - 1]] ? N[0][j] : 1+N[0][j - coins[i - 1]];// &#125;// &#125;// for (int j = 0; j &lt;= m; j++) &#123;// M[0][j] = M[1][j];// N[0][j] = N[1][j];// cout &lt;&lt; "(" &lt;&lt; M[0][j] &lt;&lt; "," &lt;&lt; N[0][j] &lt;&lt; ")";// &#125;// cout &lt;&lt;i&lt;&lt;" , "&lt;&lt;coins[i-1]&lt;&lt; endl;// resu[i - 1] = M[0][m];// resunum[i - 1] = N[0][m];//// &#125;// int min=-1;//// for (int i = 0; i &lt; n; i++) &#123;//// cout &lt;&lt; "(" &lt;&lt; resu[i] &lt;&lt; "," &lt;&lt; resunum[i] &lt;&lt; ")";// if (resu[i] == m) &#123;// if (min == -1)min = resunum[i];// if (resunum[i] &lt; min)min = resunum[i];// &#125;// &#125;// return min;// //&#125;int main() &#123; int n; cin &gt;&gt; n; Coins*coins = new Coins[n]; for (int i = 0; i &lt; n; i++) &#123; cin &gt;&gt; coins[i].value &gt;&gt; coins[i].num; &#125; int m; cin &gt;&gt; m; //如果要找的钱数是0，最少需要0个硬币 if (m == 0) &#123; cout &lt;&lt; 0; return 0; &#125; // qsort(coins,n,sizeof(coins[0]),comp); // int sum=0; // for(int i=0;i&lt;n;i++) // sum+=coins[i].num; // int*Mycoin=new int[sum]; // int soum=sum; // sum=coins[0].num; ///* for(int i=0,j=0;i&lt;soum;i++)&#123; // if(i==sum)&#123;sum+=coins[j+1].num;j++;&#125; // Mycoin[i]=coins[j].value; // cout &lt;&lt; Mycoin[i] &lt;&lt; " "; // &#125;*/ // int k = 0; // for (int i = 0; i&lt;n; i++) &#123; // for (int j = 0; j &lt; coins[i].num;j++) &#123; // Mycoin[k] = coins[i].value; // cout &lt;&lt; Mycoin[k] &lt;&lt; " "; // k++; // &#125; // &#125; // cout &lt;&lt; "totally" &lt;&lt; soum &lt;&lt; endl; // int**M=new int*[2]; // for(int i=0;i&lt;2;i++)M[i]=new int[m+1]; // int**N=new int*[2]; // for(int i=0;i&lt;2;i++)N[i]=new int[m+1]; // pp=Getcoin(Mycoin,m,soum,M,N); //cout &lt;&lt; pp &lt;&lt; endl; int pp; pp = GetCoin(coins, n, m); //若最终返回0即为不可能，输出-1。 if (pp != 0)cout &lt;&lt; pp; else cout &lt;&lt; -1;&#125;]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派解析域名失败导致apt-get update不可用]]></title>
    <url>%2F2017%2F10%2F31%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E8%A7%A3%E6%9E%90%E5%9F%9F%E5%90%8D%E5%A4%B1%E8%B4%A5%E5%AF%BC%E8%87%B4apt-get%20update%E4%B8%8D%E5%8F%AF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[捣鼓了一下午，查了n多网站。从删除文件到修改源，终于发现一直提示解析域名失败的原因是树莓派没有连上网orz…没有HDMI，我是通过树莓派的WiFi和电脑连上，电脑插了网线。后来把网线给了树莓派，电脑连树莓派的WiFi，电脑直接跳出了学校网络的登录界面，在电脑上面登录之后，这次开始更新软件源。 除了这个，我还按照网上各路大神的说法，改了好几个地方，这里一一记下来，以便以后查找。 删除某个目录的内容，只记得是/ect/apt/…/partial什么的 修改/etc/apt/resolv.conf的内容，数字改成8.8.8.8 修改源。据说给的源是国外源，速度慢。国内有很多源可用，网址如下：http://shumeipai.nxez.com/2013/08/31/raspbian-chinese-software-source.html 除此之外，倒是了解了一些些关于远程与vnc。实现这两者需要安装东西，并不是装了系统就可以连远程的。这个百度一下很多教程。 另外是在Linux命令行下的vi(vim)与nano编辑器。相较而言当然是nano好用。用nano编辑/etc/apt/s文件命令：sudo nano /etc/apt/s用vi编辑只需将nano改为vinano内有提示，只需按照提示来就好，而vi就比较复杂。http://shumeipai.nxez.com/2013/12/26/linux-on-vim-editor-tutorials.html该网址内容详尽。注意的是，vi命令模式不是命令行界面，按下esc并不会有界面上的变化，并不是教程错了。 Linux下的文件相关命令]]></content>
      <tags>
        <tag>OS</tag>
        <tag>物联网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[归并、快排、堆排序]]></title>
    <url>%2F2017%2F10%2F31%2F%E5%BD%92%E5%B9%B6%E3%80%81%E5%BF%AB%E6%8E%92%E3%80%81%E5%A0%86%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[归并排序算法是分治策略的一种体现。所谓分治策略就是把一个大问题分解成几个较小的部分，通过递归分别解决几个小问题，然后再在线性时间内将几个小问题的解合并成一个完整的解。 归并归并排序也是一种排序算法。问题：将n个数由从小到大地排序。归并提出的解决是： 将这列数分成两半 递归地将每一半排序 将两个有序列合并成一整列 归并排序在做串的切分时不考虑大小因素，只从中间切分。于是在合并时便需要对两个有序串做一定处理才能保证结果串的有序性。为此，一个容易想到的方法是创建一个临时的数组存储有序列。如图： 每次都比较1串与2串当前位置元素大小，将较小的那一个输出到TMP数组中当前位置并加一，将输出串的当前位置加一。直到两个串都输出完毕。这种方式所用的额外空间是O(n)，只需要O(n)次比较。 实际上，归并排序一般是递归进行一直到每个串的长度是1，主要运算过程全部落在合并上面。下面由递归式分析归并排序算法的时间复杂度。 归并时间空间复杂度分析： 设T(n)表示该算法在规模为n的输入实例上最坏的运行时间，假设n是2的整数次幂，可以得到递推公式如下： 则有递推关系： 将这些时间都加在一起即为nlog2N。即归并的时间复杂度是O(nlogn)级别。 下面将严格证明这个命题： ——————这个过程线省略———— 归并的代码： 123456789101112131415Array.prototype.mergeSort=function(s,e,b)&#123; //start,end,temp if(s==null)s=0; if(e==null)e=this.length-1; if(b==null)b=new Array(this.length); if(s&gt;=e)return; var m=(s+e)&gt;&gt;1; //取中间值 this.mergeSort(s,m,b); this.mergeSort(m+1,e,b); for(var i=s,j=s,k=m+1;i&lt;=e;++i) b[i]=this[(k&gt;e||j&lt;=m&amp;&amp;this[j]&lt;this[k])?j++:k++]; for(var i=s;i&lt;=e;++i) this[i]=b[i];&#125; 快排快排提供的解决是： 选择一个枢轴，小于枢轴的到左边，大于枢轴的到右边 对两边分别递归地使用快排 合并 与归并相似的一点是，二者都采用递归方法解决问题，但是归并的排序工作都留在了合并过程中完成，而快排正与之相反，排序工作都在分割的过程中完成。 从图中可以明显看出，快排最终得到的串就是一个有序的串。 快排时间空间复杂度： 快排算法依赖于枢轴PIVOT的选择。当选择的枢轴越接近中间值，树越接近于完全树的树高，算法效率越高，最好可达到O(nlogn)。糟糕的枢轴选择将会使效率急速下降，甚至达到O(n*n)的级别。于是，当一个串越是接近于无序，快排的效率越高。快排的平均时间复杂度是O(nlogn)，是所有排序算法中平均时间复杂度最好的算法。快排需要栈的辅助，其空间复杂度是O(logn)。 快排的代码： 12345678910111213Array.prototype.quikSort=function(s,e)&#123; //start,end if(s==null)s=0; if(e==null)e=this.length-1; if(s&gt;=e)return; this.swap((s+e)&gt;&gt;1,e); var index=s-1; for(var i=s;i&lt;=e;++i) if(this[i]&lt;=this[e]) this.swap(i,++index); this.quickSort(s,index-1); this.quickSort(index+1,e);&#125; 堆排序 堆排序数据结构结课的时候还考了呢。怕是写错了吧当时。哎陈年旧事，不提也罢。 p又查到说，堆的建立过程就是不断插入的过程，所以可能根本不是完全建好了一个堆才去改变，也许是边建边改变。（有道云笔记你的markdown真的问题还很大 堆是一种数据结构，可以看做是一棵任一孩子结点都小于（大顶堆）或都大于（小顶堆）父亲结点的完全二叉树。这样根结点总是整个堆中最大或小的结点，每次只需将根取出来即可保证有序。但是，每添加或减少一个结点都需要对堆进行必要的维护。于是，堆排序中最关键的操作就是将一个序列调整成为堆。 堆排序给出的解决方法： 将序列建成堆 迭代地取根进入有序队列并调整堆直到堆为有序序列 堆的产生过程其实就是一个调整过程，下面给出堆的调整过程（以大根堆为例）： 最终取出数字的顺序即为最终顺序。 堆排序适合于结点数较多情况下的要求前几个结点。当记录数较少时，不推荐使用堆排序。 hash表+堆排序是处理海量数据的绝佳组合。 堆排序时间空间复杂度 完全二叉树高度log(n+1)，即对每个节点进行调整的时间复杂度是O(logn)，包括建堆的时间耗费与取值和调整，整个算法时间复杂度是O(nlogn)。额外空间只有temp用来存取出的数，O(1)。 堆排序的代码 12345678910111213141516Array.prototype.heapSort=function()&#123; for(var i=1;i&lt;this.length;++i)&#123; for(var j=i,k=(j-1)&gt;&gt;1;k&gt;=0;j=k,k=(k-1)&gt;&gt;1)&#123; if(this[k]&gt;=this[j])break; this.swap(j,k); &#125; &#125; for(var i=this.length-1;i&gt;0;--i)&#123; this.swap(0,i); for(var j=0;k=(j+1)&lt;&lt;1;k&lt;=i;j=k,k=(k+1)&lt;&lt;1)&#123; if(k==i||this[k]&lt;this[k-1])--k; if(this[k]&lt;=this[j])braek; this.swap(j,k); &#125; &#125;&#125; 本篇文章中的代码部分来自于作者twobin于网址http://www.cnblogs.com/twobin的博文《排序算法性能比较》。]]></content>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在MFC中创建控制台]]></title>
    <url>%2F2017%2F10%2F31%2F%E5%9C%A8MFC%E4%B8%AD%E5%88%9B%E5%BB%BA%E6%8E%A7%E5%88%B6%E5%8F%B0%2F</url>
    <content type="text"><![CDATA[距离上次写已经一周了。浑浑噩噩的愚人节，浑浑噩噩的一周。但是依然要加油啊~要明确自己想要的是什么！开开心心地、充实地活着才最重要！加油！ 我用的VS是2015版，之前找了一些在MFC中同时显示窗口与命令行的方法，我可以用的只记住了一个，先记录如下： 在×.cpp的InitInstance()中的CWinAPPEx::InitInstance()前加上以下： 123#ifdef _DEBUG AllocConsole()#endif 以后可以使用_cprintf()打印字符串，需要在使用该函数的文件添加头文件： 1#include&lt;conio.h&gt; 切记！这个头文件必须要在#include”stdafx.h”语句之后写，否则会报错！ 使用步骤如下： 1234CString str="";USES_CONVERSION;LPSTR lp=T2A(str);_cprintf(lp); 这段代码完成了将CString类型的str输出的任务。]]></content>
      <tags>
        <tag>MFC</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux、Vim、Git常用命令——简明扼要版]]></title>
    <url>%2F2017%2F10%2F31%2Flinux%E3%80%81vim%E3%80%81git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[第一次操作系统实验课记录： 一。linux下常用命令二。vim编辑器用法命令 首先确保linux下安装有vim。可以使用上述命令sudo apt-get install vim来确定。如果已经安装了vim，会显示没有变化，否则会帮你安装vim。 然后就可以开始使用vim啦！ 你使用 vim 或vim filename.suffix 进入了vim的命令模式。 在命令模式下，不能进行文本编辑操作，需要按下A/a/I/i进入插入模式，在这个模式下可以像notepad一样进行文本编辑。 如果使用第二种命令进入vim，vim将会自动帮你确定文件类型。这样说吧，比如你创建了一个yayicpp.cpp文件进入编辑，vim将会使代码高亮、进行括号匹配等操作。 好的！进入文本编辑模式之后，你迫不及待地写了一段helloworld。代码完成之后，你觉得只打印一行helloworld不太够，你想知道有没有类似于VS环境中那种Ctrl+C和Ctrl+V的操作。 其实是有的。这个时候需要按下Esc重新进入命令模式。在命令模式下有很多功能组合，上述的复制粘贴就是其中一种，但是这里并不想以复制粘贴作为第一个被介绍的命令，让我们先从光标的移动开始。 按下h键，光标左移；j光标下移；k光标上移；l光标右移。 按下d键进入删除模式，这时按下h可删除光标左侧的一个字符，按下j可删除光标所在行以及下面一行；按下k可删除光标所在行与其上面一行；按下l可删除光标所在的字符。双击d键删除光标所在行。 按下u键可以撤销前一步所做的改变。 按下V（Shift+v）或是v可以进入到一个选择的可视化界面，同样可以使用hjkl控制选择范围，v按照字符选择，V按照行数选择。选择完成之后按下y就可以复制选定内容。按下p就可以将复制内容粘贴到选定的位置。 按两下y可以复制光标所在行。 写到这里，你觉得可以了，可以从vim里面退出来了。 在命令模式下输入冒号，进入最后行模式。在最低端出现了光标。 在光标位置输入w，保存文件。 输入q，退出。 这些命令可以进行组合wq。wq可以用x替代。 你进入了最后行模式，又不想退出了，想继续编辑。 按下Esc或者Backspace可以回到命令模式。 三。git初步使用git是一个强大的版本管理工具，是Linus写的。这个人emmmmm…技术天才有些狂妄。]]></content>
      <tags>
        <tag>Linux</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序列比较]]></title>
    <url>%2F2017%2F10%2F31%2F%E5%BA%8F%E5%88%97%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#include&lt;iostream&gt;#include&lt;string&gt;#include&lt;cstring&gt;#include&lt;stack&gt;using namespace std;class Pixel &#123;public: int a, b; Pixel(int x, int y) &#123; a = x; b = y; &#125;&#125;;int min(int a, int b, int c) &#123; if (a &lt; b) &#123; if (c &lt; a)return c; else if (c &gt;= a)return a; &#125; else &#123; if (c &lt; b)return c; else return b; &#125;&#125;int alpha(char fir, char sec) &#123; if (fir == sec)return 0; else return 1;&#125;/*int S(int m, int n, string a, string b, int beta) &#123; for (int i = 0; i &lt;= m; i++) M[i][0] = i*beta; for (int i = 0; i &lt;= n; i++) M[0][i] = i*beta; for (int i = 1; i &lt;= m; i++) for (int j = 1; j &lt;= n; j++) M[i][j] = min(alpha(a[i-1],b[j-1]) + M[i-1][j-1], beta + M[i-1][j], beta + M[i][j-1]); return M[m ][n];&#125;void back(int m, int n, string A,string B,int*result,int beta) &#123; if (m&lt;A.length()&amp;&amp;n&lt;B.length()&amp;&amp;M[m][n] +alpha(A[m],B[n])== M[m+1][n+1]) &#123; result[m] = 0; back(m + 1, n + 1, A,B,result,beta); &#125; else if (m&lt;A.length()&amp;&amp;n&lt;=B.length()&amp;&amp;M[m][n]+beta == M[m + 1][n]) &#123; result[m ] = 1; back(m +1, n, A,B,result,beta); &#125; else if(m&lt;=A.length()&amp;&amp;n&lt;B.length()) &#123; result[m] = -1; back(m, n + 1, A,B,result,beta); &#125;&#125;*/int SL(int m, int n, string a, string b, int beta) &#123; int**M = new int*[2]; M[0] = new int[b.length() + 1]; M[1] = new int[b.length() + 1]; for (int i = 0; i &lt;= n; i++) &#123; M[0][i] = beta*i; &#125; M[1][0] = beta; for (int i = 1; i &lt;= m; i++) &#123; M[1][0] = i*beta; for (int j = 1; j &lt;= n; j++) &#123; M[1][j] = min(alpha(a[i - 1], b[j - 1]) + M[0][j - 1], beta + M[1][j - 1], beta + M[0][j]); &#125; for (int j = 0; j &lt;= n; j++) &#123; M[0][j] = M[1][j]; &#125; &#125; int re = M[0][n]; delete[]M; return re;&#125;void DACA(int o,int r,int x, int y, string a, string b, stack&lt;Pixel&gt;*P,int beta) &#123; if (x-o &lt;= 1 || y-r &lt;= 2) &#123; int**M = new int*[x+1]; for(int i = 0; i &lt; x -o+ 1; i++) &#123; M[i] = new int[y -r+ 1]; M[i][0] = beta*i; &#125; for (int i = 0; i &lt;= y-r; i++) M[0][i] = i*beta; for (int i = 1; i &lt;= x-o; i++) for (int j = 1; j &lt;= y-r; j++) &#123; M[i][j] = min(alpha(a[i - 1], b[j - 1]) + M[i - 1][j - 1], beta + M[i][j - 1], beta + M[i - 1][j]); &#125; for (int i = x-o; i &gt;= 0; ) &#123; int j = y - r; for (j=y-r; j &gt;= 0; ) &#123; if (i&gt;=1&amp;&amp;j&gt;=1&amp;&amp;M[i][j] == alpha(a[i-1], b[j-1]) + M[i - 1][j - 1]) &#123; Pixel*p = new Pixel(i - 1+o, j - 1+r); P-&gt;push(*p); i = i - 1; j = j - 1; &#125; else if (i&gt;=1&amp;&amp;j&gt;=0&amp;&amp;M[i][j] == beta + M[i - 1][j]) &#123; Pixel*p = new Pixel(i - 1+o, j+r); P-&gt;push(*p); i -= 1; &#125; else if (i&gt;=0&amp;&amp;j&gt;=1&amp;&amp;M[i][j] == beta + M[i][j - 1]) &#123; Pixel*p = new Pixel(i+o, j - 1+r); P-&gt;push(*p); j -= 1; &#125; else break; &#125; if (j == 0 &amp;&amp; i == 0)break; &#125; return; &#125; string a2; cout &lt;&lt; "here:" &lt;&lt;( y-r) / 2 &lt;&lt; "," &lt;&lt; y-r &lt;&lt; endl; string b2 =b.substr((y-r) / 2 , y-r); int q[2] = &#123; 0,0 &#125;; int min; for (int i = 0; i &lt;= x-o; i++) &#123; a2 = a.substr(i, x-o); q[0]=SL(i, (y-r) / 2, a, b, 1); cout &lt;&lt; "done!"; q[0]+=SL(x-o - i, y-r - (y-r) / 2, a2, b2, 1); cout &lt;&lt; "done!" &lt;&lt; endl; if (i == 0)min = q[0]; else &#123; if (min &gt; q[0]) &#123; min = q[0]; q[1] = i; &#125; &#125; &#125; min = q[1]+o; Pixel*p = new Pixel(min, (y - r) / 2 + r); P-&gt;push(*p); DACA(o,r,min, (y-r) / 2+r, a, b, P, beta); cout &lt;&lt; "and here:" &lt;&lt; min &lt;&lt; "," &lt;&lt; x &lt;&lt; endl; a2 = a.substr(min-o , x-o); cout &lt;&lt; "???"; cout &lt;&lt; "hhhh" &lt;&lt; x - min &lt;&lt; ";;;" &lt;&lt; y - y / 2; DACA(min,(y-r)/2+r,x , y , a2, b2, P, beta);&#125;int main() &#123; string A, B; while (cin &gt;&gt; A &gt;&gt; B) &#123; stack&lt;Pixel&gt;*P = new stack&lt;Pixel&gt;(); cout &lt;&lt; SL(A.length(), B.length(), A, B,1); cout &lt;&lt; endl; DACA(0,0,A.length(), B.length(), A, B, P, 1); cout &lt;&lt; "LLLLL" &lt;&lt; endl; while(!(P-&gt;empty())) &#123; Pixel p = (P-&gt;top()); P-&gt;pop(); cout &lt;&lt; "(" &lt;&lt; p.a &lt;&lt; "," &lt;&lt; p.b &lt;&lt; ")"; &#125; delete P; cout &lt;&lt; "what!" &lt;&lt; endl; &#125;&#125;]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OS Lab1]]></title>
    <url>%2F2017%2F10%2F31%2FOS-Lab12%2F</url>
    <content type="text"><![CDATA[OSLab1：MIT的Lab1这个作业真的非常令人心烦，现在到了检查作业的紧要关头啦！]]></content>
      <tags>
        <tag>OS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[first try]]></title>
    <url>%2F2017%2F10%2F29%2Ffirst-try%2F</url>
    <content type="text"><![CDATA[yayi’s new blogwhat !你为什么这么多事啊！？]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F10%2F29%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
